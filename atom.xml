<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tim Mattison]]></title>
  <link href="http://blog.timmattison.com/atom.xml" rel="self"/>
  <link href="http://blog.timmattison.com/"/>
  <updated>2014-01-02T18:57:35-05:00</updated>
  <id>http://blog.timmattison.com/</id>
  <author>
    <name><![CDATA[Tim Mattison]]></name>
    <email><![CDATA[tim (at) mattison (dot) org]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Installing Unison on a Synology NAS]]></title>
    <link href="http://blog.timmattison.com/archives/2014/01/02/installing-unison-on-a-synology-nas/"/>
    <updated>2014-01-02T18:53:31-05:00</updated>
    <id>http://blog.timmattison.com/archives/2014/01/02/installing-unison-on-a-synology-nas</id>
    <content type="html"><![CDATA[<p>This is all business.  I wrote this up a long time ago and never got a chance to put it online.  Enjoy!</p>

<p>First, install SSH:</p>

<ol>
<li>Go to Synology web GUI</li>
<li>Open Control Panel</li>
<li>Click &ldquo;Terminal&rdquo;</li>
<li>Check &ldquo;Enable SSH service&rdquo;</li>
<li>Click &ldquo;Apply&rdquo;</li>
</ol>


<p>Set up your users so they have a home directory:</p>

<ol>
<li>Go to Synology web GUI</li>
<li>Open Control Panel</li>
<li>Click &ldquo;User&rdquo;</li>
<li>Click &ldquo;User Home&rdquo; button at the top</li>
<li>Check &ldquo;Enable user home service&rdquo;</li>
<li>Select the volume on which the home directories should be stored</li>
<li>Click &ldquo;OK&rdquo;</li>
</ol>


<p>Connect via SSH and make sure it works:</p>

<ol>
<li>ssh as admin to the Synology box.  Use the same password you use as admin on the web GUI.  <strong>If you leave it blank, even if your password is blank, it will always fail.  You must have a non-blank password!</strong> This is to protect against your device getting compromised from having no password.</li>
<li>If you get the &ldquo;Diskstation>&rdquo; prompt then it is working</li>
</ol>


<p>Connect via SSH and make sure non-admin/non-root logins work:</p>

<ol>
<li>Connect as a regular user.  If you get permission denied SSH back in as root and change the desired user&rsquo;s shell from /sbin/nologin to /bin/ash in /etc/passwd.</li>
<li>Try logging in again as that user</li>
</ol>


<p>Bootstrap for ipkg:</p>

<ol>
<li><a href="http://forum.synology.com/wiki/index.php/Overview_on_modifying_the_Synology_Server,_bootstrap,_ipkg_etc">http://forum.synology.com/wiki/index.php/Overview_on_modifying_the_Synology_Server,_bootstrap,_ipkg_etc</a></li>
<li>Log in as root</li>
<li>If you have DSM 4 or greater:</li>
<li> Edit /root/.profile and comment out the lines that set and export the PATH variable</li>
<li> Log out</li>
<li> Log back in as root</li>
<li>Run &ldquo;ipkg&rdquo;.  You should see the options come up and not an error message that ipkg can&rsquo;t be found.</li>
</ol>


<p>Compile and install Unison &ndash; <a href="http://www.multigesture.net/articles/how-to-compile-unison-for-a-synology-ds212/">http://www.multigesture.net/articles/how-to-compile-unison-for-a-synology-ds212/</a>
This error is expected:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>if [ -f `which etags` ]; then \
</span><span class='line'>    etags *.mli */*.mli *.ml */*.ml */*.m *.c */*.c *.txt \
</span><span class='line'>          ; fi
</span><span class='line'>/bin/sh: etags: not found
</span><span class='line'>make[1]: [tags] Error 127 (ignored)</span></code></pre></td></tr></table></div></figure>


<p>Good luck and post success stories and issues in the comments and I&rsquo;ll help if I can.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Compiling Bitcoind on the BeagleBone Black]]></title>
    <link href="http://blog.timmattison.com/archives/2014/01/02/compiling-bitcoind-on-the-beaglebone-black/"/>
    <updated>2014-01-02T18:29:05-05:00</updated>
    <id>http://blog.timmattison.com/archives/2014/01/02/compiling-bitcoind-on-the-beaglebone-black</id>
    <content type="html"><![CDATA[<p>I am running Debian on a BeagleBone Black at home as a toy server/sandbox.  One application that I thought would be interesting to run on it was the standard Bitcoin client.  I think it is a bit strange that the latest version of the Bitcoin client (0.8.6 at the time I wrote this) doesn&rsquo;t use &ldquo;configure&rdquo; like most other Linux/Unix applications because it leads to having to track down dependencies during a build rather than before them.  On a normal system this might not be a big deal because Bitcoin compiles in just a minute or so.  On a smaller device like the BeagleBone Black though it means you&rsquo;ll end up checking in on it periodically over a long period of time only to find that it needs another dependency.</p>

<p>In their defense the github version DOES use a &ldquo;configure&rdquo; script.  I found that out after going through a manual build on 0.8.6 so for completeness I&rsquo;ll show how to compile both and you can use whichever one suits your needs.  The configure script on a lean device like the BeagleBone Black still takes quite a while to run though so this should get you through doing the process just once.</p>

<p>You can use either the current stable version today which is 0.8.6 or you can use the bleeding edge github source.  I would recommend 0.8.6 if you want something that is as stable as possible.  When compiling from source you should keep in mind that your build may not be compatible with old wallet formats.</p>

<p>I cannot stress this enough &ndash; IF YOU HAVE AN OLD WALLET YOU ARE BEST OFF USING THE OFFICIAL BINARIES INSTEAD OF BUILDING FROM SOURCE!</p>

<h1>Using version 0.8.6</h1>

<p>If you want to use version 0.8.6 here&rsquo;s what you need to do:</p>

<ul>
<li>Install the necessary dependencies</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo apt-get install g++ libboost-dev libdb-dev
</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p>Download the <a href="http://sourceforge.net/projects/bitcoin/files/Bitcoin/bitcoin-0.8.6/bitcoin-0.8.6-linux.tar.gz/download">Bitcoin 0.8.6 source</a></p></li>
<li><p>Extract the source</p></li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>tar xzvf bitcoin-0.8.6-linux.tar.gz
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Remove the binaries, these are Intel binaries and won&rsquo;t work on the BeagleBone Black anyway</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>rm bitcoin-0.8.6-linux/bin/*/*
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Go to the src directory</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">cd </span>bitcoin-0.8.6-linux/src/src
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Build the source</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>make -f makefile.unix
</span></code></pre></td></tr></table></div></figure>


<p>If you follow these instructions you should get Bitcoin up and running in just one build cycle.  Here are the commands that you should run:</p>

<h1>Using the latest Bitcoin development version (NOT RECOMMENDED!)</h1>

<p>NOTE: I do not recommend that you use this version.  Currently Debian does not have libdb4.8 in its default repository and the Bitcoin client requires it to maintain compatibility with existing wallet files.</p>

<p>If you want to use the latest development version here&rsquo;s what you need to do:</p>

<ul>
<li>Install the necessary dependencies</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo apt-get install g++ libboost-dev libdb-dev git automake pkg-config
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Clone the Bitcoin repository</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git clone https://github.com/bitcoin/bitcoin.git
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Configure the source</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">cd </span>bitcoin
</span><span class='line'>./autogen.sh
</span><span class='line'>./configure --with-incompatible-bdb
</span><span class='line'>make
</span></code></pre></td></tr></table></div></figure>


<p>Now you&rsquo;ll have the bitcoind executable sitting on your BeagleBone Black.  When you try to run it the first time it will complain that some variables aren&rsquo;t set and that your config is incomplete.  The output will look something like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>debian@arm:~/bitcoin/src/src<span class="nv">$ </span>./bitcoind
</span><span class='line'>Error: To use bitcoind, you must <span class="nb">set </span>a rpcpassword in the configuration file:
</span><span class='line'>/home/debian/.bitcoin/bitcoin.conf
</span><span class='line'>It is recommended you use the following random password:
</span><span class='line'><span class="nv">rpcuser</span><span class="o">=</span>bitcoinrpc
</span><span class='line'><span class="nv">rpcpassword</span><span class="o">=</span>XxXXxXXxxxXxXXxXXXXXXxxxxXxxxxxXxXXXXXxXxxXx
</span><span class='line'><span class="o">(</span>you <span class="k">do </span>not need to remember this password<span class="o">)</span>
</span><span class='line'>The username and password MUST NOT be the same.
</span><span class='line'>If the file does not exist, create it with owner-readable-only file permissions.
</span><span class='line'>It is also recommended to <span class="nb">set </span>alertnotify so you are notified of problems;
</span><span class='line'><span class="k">for </span>example: <span class="nv">alertnotify</span><span class="o">=</span><span class="nb">echo</span> %s | mail -s <span class="s2">&quot;Bitcoin Alert&quot;</span> admin@foo.com
</span></code></pre></td></tr></table></div></figure>


<p>What you&rsquo;ll need to do is put these values into the ~/.bitcoin/bitcoin.conf.  Then you can restart bitcoind and it&rsquo;ll run and start grabbing the blockchain.  IT IS INCREDIBLY IMPORTANT THAT YOU DO NOT COPY THE VALUES THAT I PUT HERE.  Your file will look like this (except the rpcpassword will be whatever bitcoind told you):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">rpcuser</span><span class="o">=</span>bitcoinrpc
</span><span class='line'><span class="nv">rpcpassword</span><span class="o">=</span>XxXXxXXxxxXxXXxXXXXXXxxxxXxxxxxXxXXXXXxXxxXx
</span></code></pre></td></tr></table></div></figure>


<p>This password gives someone complete access to your bitcoind instance.  If you store money there and use the rpcpassword value that I put above you can and probably will lose it.</p>

<p>Unless you have a giant SD card on your BeagleBone Black you&rsquo;ll probably want to put your blockchain on a different disk.  I have my Synology home directory mounted on my BeagleBone Black via NFS (as explained in another post).  It is mounted at ~/synology.  In order to make sure my blockchain is on my Synology I did the following:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mkdir ~/synology/bitcoind
</span><span class='line'>mv ~/.bitcoin/blocks ~/.bitcoin/chainstate ~/.bitcoin/database ~/.bitcoin/db.log ~/.bitcoin/debug.log ~/synology/bitcoind/
</span><span class='line'>ln -s ~/synology/bitcoind/blocks ~/.bitcoin/blocks
</span><span class='line'>ln -s ~/synology/bitcoind/chainstate ~/.bitcoin/chainstate
</span><span class='line'>mkdir -p ~/synology/bitcoind/database
</span><span class='line'>ln -s ~/synology/bitcoind/database ~/.bitcoin/database
</span><span class='line'>ln -s ~/synology/bitcoind/db.log ~/.bitcoin/db.log
</span><span class='line'>ln -s ~/synology/bitcoind/debug.log ~/.bitcoin/debug.log
</span></code></pre></td></tr></table></div></figure>


<p>You may get a few errors about files not existing when you run this.  This is normal and you should try to proceed and see if it works for you.  I was very careful here to not put bitcoin.conf or the wallet.dat file on the Synology.  You should also avoid putting those files on there.  Since the remote file system is potentially a shared resource an attacker can get into that and modify or steal data.  It&rsquo;s best to keep the wallet.dat, peers.dat, and bitcoin.conf local to your BeagleBone Black.</p>

<p>At this point you can restart bitcoind.  I did this in a screen session rather than make it a true service since I&rsquo;m still playing around with it.  Once I set it up as a service I&rsquo;ll post an update and include that information as well.  I am a bit skeptical that it will stay stable since after just a few hours it is already taking up 50% of the BeagleBone Black&rsquo;s RAM.  I guess I&rsquo;ll just have to wait and see.</p>

<p>Periodically check your free file system space and make sure that the blockchain isn&rsquo;t on your SD card.  In my case I can do this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># Show the amount of free space on my SD card</span>
</span><span class='line'>du -sh /
</span><span class='line'>
</span><span class='line'><span class="c"># Show the amount of free space on my Synology NAS</span>
</span><span class='line'>du -sh ~/synology/
</span></code></pre></td></tr></table></div></figure>


<p>Good luck and post in the comments if this helps you out or if you need any assistance.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Connecting a BeagleBone Black to a Synology NAS With NFS]]></title>
    <link href="http://blog.timmattison.com/archives/2014/01/02/connecting-a-beaglebone-black-to-a-synology-nas-with-nfs/"/>
    <updated>2014-01-02T06:52:06-05:00</updated>
    <id>http://blog.timmattison.com/archives/2014/01/02/connecting-a-beaglebone-black-to-a-synology-nas-with-nfs</id>
    <content type="html"><![CDATA[<p>I originally purchased my Synology NAS because of all of the packages that it offers and the fact that it was supposed to give me one less thing to manage.  After a few failed attempts at getting OpenVPN to work and some Plex issues I decided that I needed to have something a little less opaque that I could install different packages on.  I wanted something small, low power, and solid state so I decided to use the BeagleBone Black.</p>

<p>I tried using both CIFS/Samba and sshfs to mount the Synology&rsquo;s filesystem but both of them had extremely strange issues with the applications I planned on using.  I wouldn&rsquo;t recommend either of them for production work involving Linux and NAS.  Often it would tell me that files didn&rsquo;t exist even though I could clearly see they were there.  After moving to NFS none of this happened anymore.</p>

<p>NOTE: Security here is all done by IP address so this is not suitable for a sensitive environment!</p>

<p>Here are the steps I went through to connect my first enable and configure NFS on my Synoloy NAS:</p>

<ol>
<li>Make sure your BeagleBone Black has a static IP and that you know what it is</li>
<li>Log into the Synology web GUI as an admin</li>
<li>Click &ldquo;Control Panel&rdquo;</li>
<li>Click &ldquo;Win/Mac/NFS&rdquo;</li>
<li>Click &ldquo;NFS Service&rdquo;</li>
<li>Make sure that &ldquo;Enable NFS&rdquo; and &ldquo;Enable NFSv4 support&rdquo; are checked</li>
<li>Click the link that says &ldquo;Shared Folder&rdquo;</li>
<li>Find and select the directory you want to share.  In my case it was my user account&rsquo;s home directory so I selected &ldquo;homes&rdquo;.</li>
<li>Click the &ldquo;Privileges&rdquo; drop down</li>
<li>Select &ldquo;NFS Privileges&rdquo;</li>
<li>Click &ldquo;Create&rdquo;</li>
<li>Enter your BeagleBone Black&rsquo;s IP address into the &ldquo;Hostname or IP&rdquo; field</li>
<li>Make sure that the &ldquo;Privilege&rdquo; field is set to &ldquo;Read/Write&rdquo;</li>
<li>Make sure that the &ldquo;Root squash&rdquo; field is set to &ldquo;No mapping&rdquo;</li>
<li>Make sure that &ldquo;Enable asynchronous&rdquo; is checked</li>
<li>Make sure that &ldquo;Allow connections from non-privileged ports (ports higher than 1024&rdquo; is NOT checked</li>
<li>Click &ldquo;OK&rdquo; in the &ldquo;Create an NFS rule&rdquo; popup</li>
<li>Take note of the &ldquo;Mount path&rdquo; field in the &ldquo;Edit NFS privileges of homes&rdquo; window, you&rsquo;ll need this later</li>
<li>Click &ldquo;OK&rdquo; in the &ldquo;Edit NFS privileges of homes&rdquo; window</li>
<li>Log out of the Synology web GUI</li>
</ol>


<p>Now you have your Synology all set up to accept NFS connections from your BeagleBone Black.  Here are the steps to do that:</p>

<ol>
<li>Connect to your BeagleBone Black via SSH as a user that can run sudo</li>
<li>Install the NFS utilities and client by running &ldquo;sudo apt-get install nfs-common&rdquo;</li>
<li>Go to your home directory</li>
<li>Create a directory for your Synology mount point.  I chose &ldquo;synology&rdquo;.</li>
<li>Mount your directory using the &ldquo;Mount path&rdquo; from above along with any additional paths to get you to the desired directory you want.  My mount path was &ldquo;/volume1/homes&rdquo; but I wanted &ldquo;/volume1/homes/tim&rdquo; for my actual home directory.  The command I used was &ldquo;sudo mount -t nfs synology:/volume1/homes/tim /home/tim/synology/&rdquo;</li>
<li>Test out your newly mounted Synology NAS</li>
</ol>


<p>You can now add this to your /etc/fstab file if you&rsquo;d like but I prefer to keep it in a script.  I have a script called mount-nfs.sh in my home directory that I run when necessary since I&rsquo;m still testing out NFSv4.  Once I get it stable I&rsquo;ll make sure that I get it to mount automatically the proper way and update this post.</p>

<p>Good luck!  Post any success stories or issues in the comments.  I&rsquo;ll do my best to help out if possible.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reading Resources in Java]]></title>
    <link href="http://blog.timmattison.com/archives/2013/12/30/reading-resources-in-java/"/>
    <updated>2013-12-30T11:02:52-05:00</updated>
    <id>http://blog.timmattison.com/archives/2013/12/30/reading-resources-in-java</id>
    <content type="html"><![CDATA[<p>Reading resources in Java has always been a mystery to me.  It&rsquo;s not because I don&rsquo;t understand how to do it but more that I do it so infrequently that
I always forget what I need to do.  In Java&rsquo;s defense once you understand what you need to do it&rsquo;s actually very simple.  I&rsquo;ve run into this issue
many times in a professional context.  Usually it is just trying to load a simple resource so I can keep it inside the JAR file I&rsquo;m delivering to a
client.</p>

<p>The sixty second explanation of how to do this is as follows.  If you&rsquo;re using an IDE and have a standard project layout you can simply follow these steps:</p>

<ol>
<li>Under your src/main directory create a new directory called &ldquo;resources&rdquo;</li>
<li>Put your static file in the &ldquo;resources&rdquo; directory.  For example, &ldquo;jquery-1.10.2.min.js&rdquo; if you want to serve this file from an embedded web server.</li>
<li>When you want to load the file do this:</li>
</ol>


<p>The long way:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">URL</span> <span class="n">jqueryUrl</span> <span class="o">=</span> <span class="n">getClass</span><span class="o">().</span><span class="na">getResource</span><span class="o">(</span><span class="s">&quot;/jquery-1.10.2.min.js&quot;</span><span class="o">);</span>
</span><span class='line'><span class="n">InputStream</span> <span class="n">jqueryInputStream</span> <span class="o">=</span> <span class="n">jqueryUrl</span><span class="o">.</span><span class="na">openStream</span><span class="o">();</span>
</span><span class='line'><span class="n">String</span> <span class="n">jquery</span> <span class="o">=</span> <span class="n">IOUtils</span><span class="o">.</span><span class="na">toString</span><span class="o">(</span><span class="n">jqueryInputStream</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>The short way:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">String</span> <span class="n">jquery</span> <span class="o">=</span> <span class="n">IOUtils</span><span class="o">.</span><span class="na">toString</span><span class="o">(</span><span class="n">getClass</span><span class="o">().</span><span class="na">getResource</span><span class="o">(</span><span class="s">&quot;/jquery-1.10.2.min.js&quot;</span><span class="o">).</span><span class="na">openStream</span><span class="o">());</span>
</span></code></pre></td></tr></table></div></figure>


<p>And that&rsquo;s it!  I recommend the long way because you can step through it if the file isn&rsquo;t found and get some insight into what happened rather than
just getting an opaque NullPointerException.  If you don&rsquo;t have the file in the directory, or you have the wrong file name, after attempting to get
the jqueryUrl it&rsquo;ll just be NULL.  If that happens just check to make sure that you have the file name correct, resources is spelled correctly, and
it is in the right location.</p>

<p>Good luck!  Post in the comments if you find this useful or if you have trouble.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Migrated From Wordpress to Octopress]]></title>
    <link href="http://blog.timmattison.com/archives/2013/12/30/migrated-from-wordpress-to-octopress/"/>
    <updated>2013-12-30T10:10:22-05:00</updated>
    <id>http://blog.timmattison.com/archives/2013/12/30/migrated-from-wordpress-to-octopress</id>
    <content type="html"><![CDATA[<p>I finally got tired of dealing with Wordpress and all of the baggage that comes along with it.  I&rsquo;ve migrated to Octopress and it feels nice.
We&rsquo;ll see if it makes me post any more or not though.  It appears that this is my only post for the entire year so far.</p>

<p>If you can recommend any themes that look a bit nicer than this please post in the comments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coming Soon: How to Easily Enable PoE (Power Over Ethernet) in Your Homebrew Projects]]></title>
    <link href="http://blog.timmattison.com/archives/2012/05/25/coming-soon-how-to-easily-enable-poe-power-over-ethernet-in-your-homebrew-projects/"/>
    <updated>2012-05-25T09:14:20-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/05/25/coming-soon-how-to-easily-enable-poe-power-over-ethernet-in-your-homebrew-projects</id>
    <content type="html"><![CDATA[<p>UPDATE 2013-12-30: This is on hold permanently.  I never ended up having the time (this post is from way over a year ago) and PoE just isn&rsquo;t on my radar lately as I&rsquo;ve shifted more towards security and software testing.</p>

<p>PoE (<a href="http://en.wikipedia.org/wiki/Power_over_Ethernet">Power over Ethernet</a>) is an exciting and promising technology.  I think that a lot of people take it for granted now that you can plug your IP phone into an Ethernet jack and it gets data and power over that single connection.  It&rsquo;s easy to forget that your IP phone isn&rsquo;t a real landline with convenience like this.</p>

<p>One thing that has been frustrating about PoE over the years is not supply side availability but consumption side hardware for hobbyists.  You can find very cheap PoE switches online and that&rsquo;s great for all of your pre-fab gadgets (IP phones, WiFi access points, even video cameras) but implementing PoE in your own projects has been tough to say the least.</p>

<p>There is now <a href="http://canakit.webstorepowered.com/Arduino-Ethernet-Power-over-Ethernet-PoE-Module/dp/B005D22FR6">an Arduino board that supports PoE</a> but if you&rsquo;re using an existing Arduino board (Mega, etc), a Netduino Plus, or a single board computer you are still out of luck if you want to use PoE so far.  Don&rsquo;t get me wrong you can do it but it takes significant electronics experience, a multitude of components, and even some luck since not all PoE mid-span devices follow the standard perfectly.  When using PoE with a non-compliant mid-span device you will certainly run into grounding issues that can range from a minor hassle to project killing either from noise or <a href="http://en.wikipedia.org/wiki/Magic_smoke">magic smoke</a>.  I haven&rsquo;t experienced magic smoke yet but I&rsquo;ve definitely seen issues with noise and FTDI boards connecting and disconnecting from USB when hooked into a PoE circuit powered by a non-compliant device.</p>

<p>As far as the mid-span devices go you&rsquo;ll have to do your homework to find one that is compliant and that might even mean buying a few devices to test them out.  So far I&rsquo;ve had some luck with a <a href="http://www.phihong.com/html/psa16u-480_15_4w_1-port_poe.html">Phihong PSA16U-480</a> although there have been a few times where I&rsquo;ve gotten the network TX/RX to stop working while the PoE portion still works.  I never did conclude whether it was a circuit problem, a Phihong problem, or a cabling problem though so I would say that so far the Phihong has been the best.  On the other hand my <a href="http://www.intellinet-network.com/en-US/products/6897-power-over-ethernet-poe-injector">Intellinet 524179</a> has consistently caused my <a href="http://www.sparkfun.com/products/9873">Sparkfun FTDI basic module</a> to disconnect from USB each time power is disconnected.  The Phihong does not do that and I have concluded that it must be a grounding problem.  To be clear my FTDI module in this case is being used to connect to a circuit that is powered by PoE but the FTDI module derives its power from a USB port, not the PoE adapter, on a computer that is connected to the same ground as the Intellinet 524179.  Because of this I would steer clear of this mid-span device if possible.</p>

<p>Now a company called <a href="http://silvertel.com">Silvertel</a> has released <a href="http://www.silvertel.com/poe_products.htm">a line of PoE modules</a> that finally make adding PoE to your circuits a much easier proposition.  With these new modules you only need 1 or 2 extra components to get yourself up and running which is a far cry from the 10 or more components I&rsquo;ve seen in previous designs.  Naturally you&rsquo;ll need to have direct, board level access to the magnetics for the Ethernet connection so it won&rsquo;t work in all circuits.  There are some embedded modules that don&rsquo;t expose the two taps needed on the TX and RX coil to make this all work so those modules will still be unable to use PoE right in their circuitry.</p>

<p>However, if you find that you&rsquo;re working with a module that doesn&rsquo;t have the proper taps you can always get a <a href="http://www.cisco.com/en/US/prod/collateral/voicesw/ps6788/phones/ps10042/ps10044/data_sheet_c78-502433.html">PoE splitter</a>.  These devices extract PoE power and either provide 48V or some lower regulated voltage and put that into a standard barrel connector.  It&rsquo;s not elegant but it works.  You must avoid any hacks where you put raw voltage on spare Ethernet lines as tempting as they might be.  Some people can get away with it during testing but there&rsquo;s always the risk that you&rsquo;ll fry something when you unexpectedly hook up the wrong port.  Do yourself a favor and stick with the standards for consistency and safety.</p>

<p>I&rsquo;m hoping to get some time next week to try Silvertel&rsquo;s modules out.  Once I do I&rsquo;ll be posting my results and, if I&rsquo;m successful, some information on how to convert your Netduino Plus into a Netduino Plus with PoE.  Either way I&rsquo;ll post updates next week and keep everyone up to date on my progress.  I expect that if it works PoE will be a major player in most or all of my future projects.</p>

<p>What projects do you have that you would like to add PoE to?  What has stopped you from doing it so far other than a lack of time?  Post in the comments and let me know.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How-To: Fix VMware Kernel Module Compile Issues With VMware Workstation 8.0.3 and Linux Kernel 3.2.0]]></title>
    <link href="http://blog.timmattison.com/archives/2012/05/04/how-to-fix-vmware-kernel-module-compile-issues-with-vmware-workstation-8-0-3-and-linux-kernel-3-2-0/"/>
    <updated>2012-05-04T09:06:30-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/05/04/how-to-fix-vmware-kernel-module-compile-issues-with-vmware-workstation-8-0-3-and-linux-kernel-3-2-0</id>
    <content type="html"><![CDATA[<p><strong>Update 2012-06-16</strong>: This still happens on the 8.0.4 update so change the values that read &ldquo;8.0.3&rdquo; to &ldquo;8.0.4&rdquo; if you are using 8.0.4.  Also, if you have patched previously and try to run the script again after an upgrade you need to remove a file called &ldquo;/usr/lib/vmware/modules/source/.patched&rdquo; first.  The script will let you know that it won&rsquo;t patch because it has already been done if you forget.  Just delete it and re-run the script.</p>

<p>Today I upgraded to VMware Workstation 8.0.3 and immediately I ran into the following error message:</p>

<pre><code>make[1]: Entering directory `/usr/src/linux-headers-3.2.0-2-amd64'
  CC [M]  /tmp/vmware-root/modules/vmnet-only/userif.o
  CC [M]  /tmp/vmware-root/modules/vmnet-only/netif.o
  CC [M]  /tmp/vmware-root/modules/vmnet-only/filter.o
/tmp/vmware-root/modules/vmnet-only/userif.c: In function ‘VNetCsumCopyDatagram’:
/tmp/vmware-root/modules/vmnet-only/userif.c:520:3: error: incompatible type for argument 1 of ‘kmap’
/usr/src/linux-headers-3.2.0-2-common/include/linux/highmem.h:48:21: note: expected ‘struct page *’ but argument is of type ‘const struct &lt;anonymous&gt;’
/tmp/vmware-root/modules/vmnet-only/userif.c:523:3: error: incompatible type for argument 1 of ‘kunmap’
/usr/src/linux-headers-3.2.0-2-common/include/linux/highmem.h:54:20: note: expected ‘struct page *’ but argument is of type ‘const struct &lt;anonymous&gt;’
/tmp/vmware-root/modules/vmnet-only/netif.c: In function ‘VNetNetIfSetup’:
/tmp/vmware-root/modules/vmnet-only/netif.c:134:7: error: unknown field ‘ndo_set_multicast_list’ specified in initializer
/tmp/vmware-root/modules/vmnet-only/netif.c:134:7: warning: initialization from incompatible pointer type [enabled by default]
/tmp/vmware-root/modules/vmnet-only/netif.c:134:7: warning: (near initialization for ‘vnetNetifOps.ndo_validate_addr’) [enabled by default]
make[4]: *** [/tmp/vmware-root/modules/vmnet-only/userif.o] Error 1
make[4]: *** Waiting for unfinished jobs....
make[4]: *** [/tmp/vmware-root/modules/vmnet-only/netif.o] Error 1
</code></pre>

<p>After lots of Googling I found <a href="http://weltall.heliohost.org/wordpress/2012/01/26/vmware-workstation-8-0-2-player-4-0-2-fix-for-linux-kernel-3-2-and-3-3/">a blog post with a patch for kernels 3.2.0 and 3.3.0</a>.  Unfortunately when I tried to run the patch it failed and said:</p>

<pre><code>Sorry, this script is only for VMWare WorkStation 8.0.2 or VMWare Player 4.0.2. Exiting
</code></pre>

<p>In order to fix this open up the script after you download it and change the line this line:</p>

<pre><code>vmreqver=8.0.2
</code></pre>

<p>To this:</p>

<pre><code>vmreqver=8.0.3
</code></pre>

<p>Re-run the script and you should be good to go.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tips for Debugging Spring's @Transactional Annotation]]></title>
    <link href="http://blog.timmattison.com/archives/2012/04/19/tips-for-debugging-springs-transactional-annotation/"/>
    <updated>2012-04-19T10:31:14-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/04/19/tips-for-debugging-springs-transactional-annotation</id>
    <content type="html"><![CDATA[<p>For over a week now I&rsquo;ve been cleaning up some legacy code that uses Spring and Hibernate to persist and process data in a SQL database.  The code works but it doesn&rsquo;t follow the strict philosophy of service oriented architecture in the sense that there are several places that Spring and Hibernate weren&rsquo;t doing what they were expected to do and a few workarounds had to be implemented.  Since we were bringing more programmers on board I wanted to make sure that everything played by the rules and was easy to update so I had to learn a lot that I had glossed over in the past.</p>

<p>With some creative Googling I found two invaluable resources that I need to give credit to:</p>

<ul>
<li><p><a href="http://java.dzone.com/articles/monitoring-declarative-transac?page=0,1">Monitoring Declarative Transactions in Spring</a></p></li>
<li><p><a href="http://stackoverflow.com/questions/3037006/starting-new-transaction-in-spring-bean">Starting new transactions in Spring bean</a></p></li>
</ul>


<p>Here&rsquo;s what I distilled out of everything I went through:</p>

<ol>
<li><p> @Transactional annotations only work on public methods.  If you have a private or protected method with this annotation there&rsquo;s no (easy) way for Spring AOP to see the annotation.  It doesn&rsquo;t go crazy trying to find them so make sure all of your annotated methods are public.</p></li>
<li><p> Transaction boundaries are only created when properly annotated (see above) methods are called through a Spring proxy.  This means that you need to call your annotated method directly through an @Autowired bean or the transaction will never start.  If you call a method on an @Autowired bean that isn&rsquo;t annotated which itself calls a public method that is annotated <strong><em>YOUR ANNOTATION IS IGNORED</em></strong>.  This is because Spring AOP is only checking annotations when it first enters the @Autowired code.</p></li>
<li><p> Never blindly trust that your @Transactional annotations are actually creating transaction boundaries.  When in doubt test whether a transaction really is active (see below)</p></li>
</ol>


<p>My first problem was that the code was annotated improperly like this:</p>

<pre><code>/**
 * This code example is BAD code, do not use it!
 */
class NonWorkingMyClass {

  @Autowired
  protected MyService myService;

  public void calledFirst() {
    // Do some setup work...

    // Call our internal method
    calledSecond();
  }

  @Transactional
  private void calledSecond() {
    MyObject myObject = myService.retrieveLatest();

    // Update some object fields
    myObject.setName("New Name");
  }
}
</code></pre>

<p>In this case someone would call NonWorkingMyClass.calledFirst(), it would then call calledSecond() and try to update the name field.  This works if your XML configuration is set up properly but it will not be in a transaction.  This can cause concurrency issues that won&rsquo;t show up until it&rsquo;s really inconvenient.</p>

<p>Here&rsquo;s the working version of that code:</p>

<pre><code>/**
 * This code example works
 */
class WorkingMyClass {

  @Autowired
  protected MyService myService;

  @Transactional
  public void calledFirst() {
    // Do some setup work...

    // Call our internal method
    calledSecond();
  }

  private void calledSecond() {
    MyObject myObject = myService.retrieveLatest();

    // Update some object fields
    myObject.setName("New Name");
  }
}
</code></pre>

<p>Now when someone called WorkingMyClass.calledFirst() it would do what you expect in a transaction and the transaction boundaries are properly respected.</p>

<p>This looks like a simple fix that should only take a few minutes but finding out that was the problem involved turning on lots of Spring DEBUG level logging, Googling, and actually testing to make sure the transactions were active.  Before I knew what I know now I used some code from the first site I listed to show if I was inside a transaction or not.  I was shocked and relieved when it showed that I wasn&rsquo;t because it meant the concurrency issues weren&rsquo;t due to bad programming, just bad configuration.  Here are the methods that I came up with that you can use to see if you are in a transaction and even force your code to throw an exception if it isn&rsquo;t.  This can be invaluable if someone messes up an annotation in the future or breaks your XML configuration.</p>

<p>This code belongs in a utility class that is accessible from anywhere.  There are two flags you will need to put somewhere:</p>

<p>transactionDebugging &ndash; Indicates we should do the transaction tests
verboseTransactionDebugging &ndash; Indicates we should print debug messages with the transaction tests</p>

<p>verboseTransactionDebugging has no effect if transactionDebugging is false.</p>

<pre><code>class DebugUtils {
    private static final transactionDebugging = true;
    private static final verboseTransactionDebugging = true;

    public static void showTransactionStatus(String message) {
        System.out.println(((transactionActive()) ? "[+] " : "[-] ") + message);
    }

    // Some guidance from: http://java.dzone.com/articles/monitoring-declarative-transac?page=0,1
    public static boolean transactionActive() {
        try {
            ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader();
            Class tsmClass = contextClassLoader.loadClass("org.springframework.transaction.support.TransactionSynchronizationManager");
            Boolean isActive = (Boolean) tsmClass.getMethod("isActualTransactionActive", null).invoke(null, null);

            return isActive;
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        } catch (IllegalArgumentException e) {
            e.printStackTrace();
        } catch (SecurityException e) {
            e.printStackTrace();
        } catch (IllegalAccessException e) {
            e.printStackTrace();
        } catch (InvocationTargetException e) {
            e.printStackTrace();
        } catch (NoSuchMethodException e) {
            e.printStackTrace();
        }

        // If we got here it means there was an exception
        throw new IllegalStateException("ServerUtils.transactionActive was unable to complete properly");
    }

    public static void transactionRequired(String message) {
        // Are we debugging transactions?
        if (!transactionDebugging) {
            // No, just return
            return;
        }

        // Are we doing verbose transaction debugging?
        if (verboseTransactionDebugging) {
            // Yes, show the status before we get to the possibility of throwing an exception
            showTransactionStatus(message);
        }

        // Is there a transaction active?
        if (!transactionActive()) {
            // No, throw an exception
            throw new IllegalStateException("Transaction required but not active [" + message + "]");
        }
    }
}
</code></pre>

<p>In our previous code example we could use these new methods like this:</p>

<pre><code>/**
 * This code example works
 */
class WorkingMyClass {

  @Autowired
  protected MyService myService;

  @Transactional
  public void calledFirst() {
    // Make sure we're using transactions.  Include the name of the class and method
    //   so it is easier to track down later if there are problems.
    DebugUtils.transactionRequired("WorkingMyClass.calledFirst");

    // Do some setup work...

    // Call our internal method
    calledSecond();
  }

  private void calledSecond() {
    MyObject myObject = myService.retrieveLatest();

    // Update some object fields
    myObject.setName("New Name");
  }
}
</code></pre>

<p>That&rsquo;s it.  Post in the comments if this helps you out or if you want to add to the code.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How-To: Get Verizon's Media Manager to Read Content From a Network Location]]></title>
    <link href="http://blog.timmattison.com/archives/2012/04/17/how-to-get-verizons-media-manager-to-read-content-from-a-network-location/"/>
    <updated>2012-04-17T15:45:12-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/04/17/how-to-get-verizons-media-manager-to-read-content-from-a-network-location</id>
    <content type="html"><![CDATA[<p>I ran into this problem today too when I first got FiOS installed.  Mapping a network drive won&rsquo;t work but using &ldquo;subst&rdquo; will.  I now have Media Manager reading my pictures over a network connection.  Here&rsquo;s how to do it:</p>

<ol>
<li><p> Open the start menu</p></li>
<li><p> Type &ldquo;cmd&rdquo;</p></li>
<li><p> Right click on &ldquo;cmd&rdquo; and select &ldquo;Run as administrator&rdquo;</p></li>
<li><p> Run subst like this:</p>

<p>subst DRIVE: LOCATION</p></li>
</ol>


<p>  DRIVE: will need to be a free drive letter like &ldquo;F:&rdquo;, &ldquo;G:&rdquo;, etc
  LOCATION will need to be the <a href="http://en.wikipedia.org/wiki/Path_(computing">UNC path</a>#Uniform_Naming_Convention) to your network share like this &ldquo;\myothercomputer\pictures\&rdquo;
  Don&rsquo;t forget to include the quotes if your LOCATION has spaces in it!</p>

<ol>
<li> Restart Media Manager and try to add the new virtual drive to it, it should start working right away</li>
</ol>


<p>You may need to do this on each reboot.  I never reboot this computer so I haven&rsquo;t tested it yet.  You can put these commands in a batch file to make your life easier but you&rsquo;ll need to make sure the batch file runs as an administrator.</p>

<p>Let me know in the comments if it works for you or not.  If not I can probably help work out any kinks with you.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tip: Fix "'Xterm': Unknown Terminal Type" Messages in Debian]]></title>
    <link href="http://blog.timmattison.com/archives/2012/04/12/tip-fix-xterm-unknown-terminal-type-messages-in-debian/"/>
    <updated>2012-04-12T10:32:57-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/04/12/tip-fix-xterm-unknown-terminal-type-messages-in-debian</id>
    <content type="html"><![CDATA[<p>This one has been a bit of a nuisance on newly spooled up Debian instances for me lately.  When I try to run &ldquo;top&rdquo; or &ldquo;clear&rdquo; or really anything that does something with the terminal I get the following message:</p>

<pre><code>'xterm': unknown terminal type.
</code></pre>

<p>This is because either you haven&rsquo;t installed ncurses-term (unlikely) or a symlink from /lib/terminfo/x/xterm to /usr/share/terminfo/x/xterm is missing.  To cover all possibilities do this:</p>

<pre><code>sudo apt-get install ncurses-term
sudo ln -s /lib/terminfo/x/xterm /usr/share/terminfo/x/xterm
</code></pre>

<p>Poof, your terminal works again!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How-To: Write a Netduino Driver for the Grove Chainable RGB LED]]></title>
    <link href="http://blog.timmattison.com/archives/2012/03/29/how-to-write-a-netduino-driver-for-the-grove-chainable-rgb-led/"/>
    <updated>2012-03-29T10:58:01-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/03/29/how-to-write-a-netduino-driver-for-the-grove-chainable-rgb-led</id>
    <content type="html"><![CDATA[<p>A lot of people probably look at hardware that doesn&rsquo;t come with drivers for the Netduino or Arduino and don&rsquo;t even consider picking it up if they&rsquo;re new to this scene.  In this article I&rsquo;ll show you how I wrote a driver for <a href="http://www.epictinker.com/Grove-Chainable-RGB-LED-p/com53140p.htm">Grove&rsquo;s chainable RGB LED</a> by just carefully reading the specs and experimenting.  I am no Netduino expert, I have only written a tiny bit of code for it since I got it, but this just reinforces how easy some drivers can be to write.</p>

<p>Keep in mind that my illustration of how easy it was to write this driver is not a reflection on how easy it is to write <strong><em>all</em></strong> drivers.  Some drivers take a ton of work.  Make sure you read the documentation before you buy something so you don&rsquo;t get stuck with some hardware you can&rsquo;t use.</p>

<p>My first step was to find <a href="http://www.seeedstudio.com/wiki/index.php?title=Twig_-_Chainable_RGB_LED">the documentation for the protocol for this device</a>.  I then scanned around to find the &ldquo;Communication Protocol&rdquo; and started digging.  What this showed me is that there are two connections to support the protocol for this device.  The first connection is called &ldquo;CIN&rdquo; for clock input and the second connection is called &ldquo;DIN&rdquo; for data input.  Simple enough, especially if we&rsquo;re using the standard Grove base shield and connectors.  Just hook it up and make sure you keep track of which port you&rsquo;re using and you&rsquo;re ready to start programming.  I used header 6 on my base shield so the relevant pins for me were D6 and D7.  D6 was CIN and D7 was DIN.</p>

<p>Now you&rsquo;ll see that there are six well defined bullet points explaining the basics of the protocol:</p>

<ul>
<li><p>Data needs to be ready before CIN, and DIN gets into the buffer on the rising edge of CIN.</p></li>
<li><p>First 32 bits &lsquo;0&rsquo; are Start Frame</p></li>
<li><p>Flag bit is two &lsquo;1&rsquo;</p></li>
<li><p>Calibration bits B7&#8217;,B6&#8217;;G7&#8217;,G6&#8217; and R7&#8217;,R6&#8217; are inverse codes of B7,B6;G7,G6 and R7,R6</p></li>
<li><p>Gray data MSB first, and the order is BLUE, GREEN, and RED</p></li>
<li><p>After all nodes data sent, need to seed another 32 bits &lsquo;0&rsquo; to update the data</p></li>
</ul>


<p>Let&rsquo;s step through these one by one to figure out how to send data to this device.</p>

<p>They first tell us that &ldquo;data needs to be ready before CIN, and DIN gets into the buffer on the rising edge of CIN&rdquo;.  What this really translates to for you when programming is that when you want to send a bit to the device you should set that bit on the DIN pin (either 1 or 0), then set the CIN pin to high, and then set the CIN pin back to low.  Now you&rsquo;ve sent one bit of data to the device.  Abstraction will make it so we can do this thinking once and then fall back on it later so let&rsquo;s write a function that sends one bit:</p>

<pre><code>private void sendBit(bool bit)
{
    // Get DIN into the proper state
    din.Write(bit);

    // Set the clock high
    cin.Write(true);

    // Set the clock low
    cin.Write(false);
}
</code></pre>

<p>This function makes the assumption that you&rsquo;ve defined cin and din elsewhere.  The setup for them in my case (using pins D6 and D7 as I described above) would look like this:</p>

<pre><code>// Use D6 for CIN and D7 for DIN (Grove Base Shield v1.2 header #6)
OutputPort cin = new OutputPort(Pins.GPIO_PIN_D6, false);
OutputPort din = new OutputPort(Pins.GPIO_PIN_D7, false);
</code></pre>

<p>Now they tell us that the first 32 bits are all zeroes and that this is called a start frame.  This makes me think it would be a good idea to expand our abstraction to let us send bytes and then write another function that would send this start frame.  That would look like this:</p>

<pre><code>private void sendByte(byte data)
{
    // Send the bits MSB first
    sendBit((data &amp; 0x80) == 0x80);
    sendBit((data &amp; 0x40) == 0x40);
    sendBit((data &amp; 0x20) == 0x20);
    sendBit((data &amp; 0x10) == 0x10);
    sendBit((data &amp; 0x08) == 0x08);
    sendBit((data &amp; 0x04) == 0x04);
    sendBit((data &amp; 0x02) == 0x02);
    sendBit((data &amp; 0x01) == 0x01);
}

private void sendStartFrame()
{
    // The start frame is 32 bits of zeroes
    sendByte(0);
    sendByte(0);
    sendByte(0);
    sendByte(0);
}
</code></pre>

<p>In the sendByte function I&rsquo;m taking a byte and using logical AND and equals to extract the bits one by one.  One of the next bullet points says the data is MSB first so we want to get the most significant (ie. largest value holding) bits first so that&rsquo;s how I went about sending the bits.  Now that we can send bytes sending the start frame is as easy as calling that function four times with the value 0.</p>

<p>Next they talk about flag bits.  In the protocol it shows that after the start frame there are some flag bits.  This tells us the two flag bits are both ones.  Here&rsquo;s a simple function that can do that:</p>

<pre><code>private void sendFlagBits()
{
    // The flag bits are two 1s
    sendBit(true);
    sendBit(true);
}
</code></pre>

<p>Now this part gets a bit trickier but not too bad.  They tell us that we need to send the inverse of B7, B6, G7, G6, R7, R6, followed by the actual color data itself as bytes.  B7 and B6 are the two highest bits in the blue color component, G7 and G6 are the two highest bits in the green color component, and R7 and R6 are the two highest bits in the red color component.  Sending that data with the functions we built up now is really easy.</p>

<pre><code>private void sendColorData(byte red, byte green, byte blue)
{
    // Send the inverse bits of the B7, B6, G7, G6, R7, R6
    sendBit((blue &amp; 0x80) != 0x80);
    sendBit((blue &amp; 0x40) != 0x40);
    sendBit((green &amp; 0x80) != 0x80);
    sendBit((green &amp; 0x40) != 0x40);
    sendBit((red &amp; 0x80) != 0x80);
    sendBit((red &amp; 0x40) != 0x40);

    // Send the actual colors
    sendByte((byte)blue);
    sendByte((byte)green);
    sendByte((byte)red);
}
</code></pre>

<p>We&rsquo;re almost there, there&rsquo;s only one step left!  Now we need to send the end frame.  It turns out that the end frame is the same as the start frame but to keep the code readable I did this:</p>

<pre><code>private void sendEndFrame()
{
    // The end frame is the same as the start frame
    sendStartFrame();
}
</code></pre>

<p>Now you have enough information to send a color to your device.  We should probably wrap it up so that we can make it even easier to use though.  Let&rsquo;s just think about how this is going to be used in practice.  A typical user will have a few of these LEDs chained together but for testing you might want to use just one.  We know that the protocol requires a start frame, then flag bits, then color data, and the end frame if we use a single LED but for two LEDs it looks like this:</p>

<ul>
<li><p>Send start frame</p></li>
<li><p>Send flag bits</p></li>
<li><p>Send color data</p></li>
<li><p>Send flag bits</p></li>
<li><p>Send color data</p></li>
<li><p>Send end frame</p></li>
</ul>


<p>So for our first LED we want to send the start frame, the flag bits and the color data.  For our last LED we want to send flag bits, the color data, and the end frame.  Here&rsquo;s a function that does that:</p>

<pre><code>private void setColor(byte red, byte green, byte blue, bool first, bool last)
{
    // Is this the first color?
    if (first)
    {
        // Yes, send the start frame
        sendStartFrame();
    }
    else
    {
        // No, do nothing
    }

    // Send the flag bits
    sendFlagBits();

    // Send the colors
    sendColorData(red, green, blue);

    // Is this the last color?
    if (last)
    {
        // Yes, send the end frame
        sendEndFrame();
    }
    else
    {
        // No, do nothing
    }
}
</code></pre>

<p>The extra else blocks have no impact on the executable generated so they&rsquo;re just there for clarity.  You can remove them if you want.  Now if you want to send a bunch of colors to a string of three LEDs you can do this:</p>

<pre><code>setColor(255, 0, 0, true, false);
setColor(0, 255, 0, false, false);
setColor(0, 0, 255, false, true);
</code></pre>

<p>That would set a string of three LEDs to solid red, solid green, and solid blue.  That&rsquo;s it, your driver is written!</p>

<p>Check out <a href="https://github.com/timmattison/timmattison-netduino-drivers/tree/master/drivers/chainable-rgbled-grove/chainable-rgbled-grove">my driver on Github</a> to see a few more enhancements I added.  My code has an abstraction of a color from three integers into an RGB object so it&rsquo;s easier to pass around and also has a function that can set a string of LEDs from an array of RGB objects.  There&rsquo;s some sample code as well and if you want to see the system in action check out these simple videos:</p>

<ul>
<li><p><a href="http://www.youtube.com/watch?v=cOlJoXWr_qQ">Cycling random colors</a></p></li>
<li><p><a href="http://www.youtube.com/watch?v=b5X3mvLbBf8">Cycling red, green, and blue</a></p></li>
</ul>


<p>Post in the comments and share your thoughts and project ideas.  If you use this library please let me know!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How-To: Fix Maven Errors in Eclipse When Getting Started With Heroku]]></title>
    <link href="http://blog.timmattison.com/archives/2012/03/28/how-to-fix-maven-errors-in-eclipse-when-getting-started-with-heroku/"/>
    <updated>2012-03-28T09:15:24-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/03/28/how-to-fix-maven-errors-in-eclipse-when-getting-started-with-heroku</id>
    <content type="html"><![CDATA[<p>I haven&rsquo;t used Heroku much yet but with the addition of Java to their platform I&rsquo;m starting to see it as a really interesting option.  Yesterday I watched <a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=mkmWwA0EoGg#!">a great video on how to get started with Java on Heroku</a>.  It went well until I tried converting my project to a Maven project.  Then I got this error message in all of my pom.xml files:</p>

<p><code>Plugin execution not covered by lifecycle configuration</code></p>

<p>I checked the usual places but didn&rsquo;t find a solution to the issue.  Then I decided to try adding the m2e plugin from this update site:</p>

<p><code>http://download.eclipse.org/technology/m2e/releases</code></p>

<p>After adding the plugin and restarting my IDE I got two different error messages:</p>

<p><code>maven-dependency-plugin (goals "copy-dependencies", "unpack") is not supported by m2e.</code>
<code>Project configuration is not up-to-date with pom.xml.  Run project configuration update.</code></p>

<p>The second error had a quick fix so I tried that and it worked.  Now the Java example application that uses the Play framework and the one that uses Spring MVC and Hibernate both work.  However, the ones that used JAX-RS and embedded Jetty did not.  They still showed the maven-dependency-plugin error.  The fix is to add the following XML in the build section of your pom.xml:</p>

<pre><code>&lt;pluginmanagement&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupid&gt;org.eclipse.m2e&lt;/groupid&gt;
                &lt;artifactid&gt;lifecycle-mapping&lt;/artifactid&gt;
                &lt;version&gt;1.0.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;lifecyclemappingmetadata&gt;
                        &lt;pluginexecutions&gt;
                            &lt;pluginexecution&gt;
                                &lt;pluginexecutionfilter&gt;
                                    &lt;groupid&gt;org.apache.maven.plugins&lt;/groupid&gt;
                                    &lt;artifactid&gt;maven-dependency-plugin&lt;/artifactid&gt;
                                    &lt;versionrange&gt;[1.0.0,)&lt;/versionrange&gt;
                                    &lt;goals&gt;
                                        &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                                    &lt;/goals&gt;
                                &lt;/pluginexecutionfilter&gt;
                                &lt;action&gt;
                                    &lt;ignore&gt;&lt;/ignore&gt;
                                &lt;/action&gt;
                            &lt;/pluginexecution&gt;
                        &lt;/pluginexecutions&gt;
                    &lt;/lifecyclemappingmetadata&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/pluginmanagement&gt;
</code></pre>

<p>After that you&rsquo;ll have to do the quick fix for the error &ldquo;Project configuration is not up-to-date&rdquo; again and then you&rsquo;ll be error free, at least in your pom.xml&hellip;</p>

<p>Post in the comments and let me know if it worked or if you need any help.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tip: Handle Failed Tasks Throwing "ENOENT" Errors in Hadoop]]></title>
    <link href="http://blog.timmattison.com/archives/2012/03/21/tip-handle-failed-tasks-throwing-enoent-errors-in-hadoop/"/>
    <updated>2012-03-21T19:09:48-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/03/21/tip-handle-failed-tasks-throwing-enoent-errors-in-hadoop</id>
    <content type="html"><![CDATA[<p>Today when I tried to run a new Hadoop job I got the following error:</p>

<pre><code>     [exec] 12/03/21 22:51:47 INFO mapred.JobClient: Task Id : attempt_201203212250_0001_m_000002_1, Status : FAILED
     [exec] Error initializing attempt_201203212250_0001_m_000002_1:
     [exec] ENOENT: No such file or directory
     [exec]     at org.apache.hadoop.io.nativeio.NativeIO.chmod(Native Method)
     [exec]     at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:521)
     [exec]     at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:344)
     [exec]     at org.apache.hadoop.mapred.JobLocalizer.initializeJobLogDir(JobLocalizer.java:240)
     [exec]     at org.apache.hadoop.mapred.DefaultTaskController.initializeJob(DefaultTaskController.java:216)
     [exec]     at org.apache.hadoop.mapred.TaskTracker$4.run(TaskTracker.java:1352)
     [exec]     at java.security.AccessController.doPrivileged(Native Method)
     [exec]     at javax.security.auth.Subject.doAs(Subject.java:416)
     [exec]     at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
     [exec]     at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1327)
     [exec]     at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1242)
     [exec]     at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2541)
     [exec]     at org.apac
</code></pre>

<p>It wasn&rsquo;t immediately apparent to me what file wasn&rsquo;t found from the error messages so I checked the logs, the JobTracker, my code, ran some known good jobs that also failed, basically everything I could think of.  It turns out that due to me accidentally running a script as &ldquo;root&rdquo; (don&rsquo;t worry, it was only on my desktop) that the permissions of several files in the hdfs user&rsquo;s home directory had changed ownership to &ldquo;root&rdquo;.  Because of that Hadoop was unable to create files in the /usr/lib/hadoop-0.20 directory.</p>

<p>NOTE: These steps assume you are using Hadoop 0.20.  Adjust the paths in the commands accordingly if you aren&rsquo;t.</p>

<p>If you want a quick fix try these steps (only if you take full responsibility for anything that may go wrong):</p>

<ol>
<li><p> Stop Hadoop using the stop-all.sh script as the hdfs user</p></li>
<li><p> su to the hdfs user</p></li>
<li><p> Run this:</p>

<p>chown -R hdfs:hdfs /usr/lib/hadoop-0.20 /var/*/hadoop-0.20</p></li>
<li><p> Restart Hadoop using the start-all.sh script as the hdfs user</p></li>
</ol>


<p>Now your jobs should start running again.  Post in the comments if this procedure works for you or if you need any help.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How-To: Install Perl Debugging in Eclipse on Debian/Ubuntu]]></title>
    <link href="http://blog.timmattison.com/archives/2012/03/14/how-to-install-perl-debugging-in-eclipse-on-debian-ubuntu/"/>
    <updated>2012-03-14T12:31:48-04:00</updated>
    <id>http://blog.timmattison.com/archives/2012/03/14/how-to-install-perl-debugging-in-eclipse-on-debian-ubuntu</id>
    <content type="html"><![CDATA[<p>If you&rsquo;re looking to use Eclipse as a debugger for your Perl scripts things can get a bit hairy quickly.  You need to do a lot of things to get it to be happy so let&rsquo;s step through them all rather than have you hunt for the secret sauce like I did today.</p>

<p>First, you&rsquo;ll want to <a href="http://www.epic-ide.org/download.php">add the EPIC (Eclipse Perl Integration Component) as described on the EPIC site</a>.  That will add support for creating Perl projects, syntax highlighting, and all that.</p>

<p>Next, set a breakpoint in one of your Perl scripts and try to debug it.  If you&rsquo;re unlucky you may get one of two error messages.  One error message wants you to install <a href="http://search.cpan.org/~robin/PadWalker-1.93/PadWalker.pm">PadWalker</a> which is a Perl module that handles all of the debugging niceties for Eclipse.  To install that you can either use CPAN or apt.  Using apt is as simple as:</p>

<pre><code>sudo apt-get install libpadwalker-perl
</code></pre>

<p>Once you install PadWalker restart Perl and try to debug one of your scripts again.  If it works, you&rsquo;re set.  The second possible error message is below&hellip;</p>

<p>Now, you&rsquo;ve come all this way and it still doesn&rsquo;t work.  You&rsquo;ve probably received an error message like this:</p>

<pre><code>Could not create the view: Plug-in "org.eclipse.debug.ui" was unable to instantiate class "org.eclipse.debug.internal.ui.views.variables.VariablesView".
</code></pre>

<p>If you dig deeper you&rsquo;ll see errors like this:</p>

<pre><code>java.lang.ClassCircularityError: org/eclipse/debug/internal/ui/DebugUIPlugin
</code></pre>

<p>And if you dig <em>even</em> deeper you&rsquo;ll see errors like this:</p>

<pre><code>Conflict for 'org.epic.perleditor.commands.clearMarker'
</code></pre>

<p>The fix for this was tricky to figure out so just follow these steps:</p>

<ol>
<li><p> Close Eclipse</p></li>
<li><p> Uninstall libpadwalker-perl by running</p>

<p>sudo apt-get remove &mdash;purge libpadwalker-perl</p></li>
<li><p> Restart Eclipse and try to set a breakpoint in a Perl script, it should fail (no breakpoint should appear)</p></li>
<li><p> Close Eclipse</p></li>
<li><p> Reinstall libpadwalker-perl by running</p>

<p>sudo apt-get install libpadwalker-perl</p></li>
<li><p> Restart Eclipse, set a breakpoint, and start debugging again</p></li>
</ol>


<p>At this point the variables and breakpoints should always work.  Unfortunately the expressions panel will not.  It looks like this is not supported in EPIC just yet.  But, in any case, you now have a full fledged Perl debugger so you can (mostly) stop using print statements to debug your code post mortem.</p>

<p>There are some quirks to note:</p>

<ol>
<li><p> &ldquo;Step Over&rdquo; (typically F6) does not work as expected and will step into modules.  If &ldquo;Step Return&rdquo; worked this wouldn&rsquo;t be a problem but it doesn&rsquo;t (see the next bullet point).  In this case if you are trying to step over a module you may have to back out and set a breakpoint where the execution will return to the script you want to debug.</p></li>
<li><p> &ldquo;Step Return&rdquo; (typically F7) does not work as expected.  It will usually run until your script ends or hits a breakpoint.</p></li>
<li><p> The console window will not let you run arbitrary Perl code so it&rsquo;s not a simple replacement for the expressions panel</p></li>
<li><p> Perl modules (files with a .pm extension) may not appear with syntax highlighting enabled.  If you are debugging Perl modules you may want to retool your setup and run the module as a Perl script OR have Perl load your module from a file with a .pl extension.</p></li>
</ol>


<p>Good luck.  Now clean up/fix that Perl code and post in the comments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tip: Getting the Right Static Imports Necessary for Basic JUnit Testing]]></title>
    <link href="http://blog.timmattison.com/archives/2012/03/02/tip-getting-the-right-static-imports-necessary-for-basic-junit-testing/"/>
    <updated>2012-03-02T08:02:47-05:00</updated>
    <id>http://blog.timmattison.com/archives/2012/03/02/tip-getting-the-right-static-imports-necessary-for-basic-junit-testing</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve written plenty of JUnit tests in the past but usually I&rsquo;m building onto an existing codebase of tests.  In the past few days I&rsquo;ve been playing around with Unicode and wanted to copy a code snippet from a Hadoop book to see how everything looks in the debugger.  When I entered the code I realized that I was missing some methods that I needed to complete the tests.</p>

<p>Specifically I was trying to use assertThat() and is() but didn&rsquo;t know where to find them.  After a bit of Googling I found the two static imports that I needed to copy the code without qualifying assertThat() as Assert.assertThat() and the same goes for is().  They are:</p>

<pre><code>import static org.hamcrest.CoreMatchers.is;
import static org.junit.Assert.assertThat;
</code></pre>

<p>I have to admit that org.hamcrest is a bit less obvious than I would have liked.  :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tip: A Quick Primer on Waiting on Multiple Threads in Java]]></title>
    <link href="http://blog.timmattison.com/archives/2012/02/28/tip-a-quick-primer-on-waiting-on-multiple-threads-in-java/"/>
    <updated>2012-02-28T16:32:21-05:00</updated>
    <id>http://blog.timmattison.com/archives/2012/02/28/tip-a-quick-primer-on-waiting-on-multiple-threads-in-java</id>
    <content type="html"><![CDATA[<p>Last night I was writing some code to do some performance testing on HDFS.  I noticed that single threaded performance wasn&rsquo;t anywhere near as good as I expected and my CPUs were spending most of their time idle.  I decided to add some threads into the process to see if a multi-threaded speed test would consume some of that idle CPU.  It worked as expected so I figured I would share some basic knowledge on how to I started up multiple threads, had them do their work, waited for them to finish without polling, and then recorded the total duration to calculate my statistics.</p>

<p>What you&rsquo;ll need to do first is decide what you want to do in the processing thread.  This code will go into a Java Runnable like this:</p>

<pre><code>Runnable runnable = new Runnable() {
    @Override
    public void run() {
        // Do something exciting here
    }
};
</code></pre>

<p>Next you&rsquo;ll need to decide how many threads you want to run.  If you wanted to run four threads you could do this:</p>

<pre><code>int threadCount = 4;

for (int threadLoop = 0; threadLoop &lt; threadCount; threadLoop++) {
    // XXX - Put the runnable block from above right here

    // Create a new thread
    Thread thread = new Thread(runnable);

    // Add the thread to our thread list
    threads.add(thread);

    // Start the thread
    thread.start();
}
</code></pre>

<p>That will start four threads.  It&rsquo;s best to use a variable so you can update it and use it in other places like calculating your statistics.  Now let&rsquo;s wait for all the threads to finish:</p>

<pre><code>// Loop through the threads
for (Thread thread : threads) {
    try {
        // Wait for this thread to die
        thread.join();
    } catch (InterruptedException e) {
        // Ignore this but print a stack trace
        e.printStackTrace();
    }
}
</code></pre>

<p>Finally, you&rsquo;ll want to time all of this.  I do something very simple here.  Before all of the code I do this:</p>

<pre><code>long startTime = new Date().getTime();
</code></pre>

<p>After all of the code I do this:</p>

<pre><code>long endTime = new Date().getTime();
long durationInMilliseconds = endTime - startTime;
</code></pre>

<p>With all of that in place you can now measure how long your code ran and then calculate important metrics about it.  For example, if this code did 10,000 operations per thread and ran with 4 threads you would then take the duration and divide that by 40,000 and you&rsquo;d get an idea of how many milliseconds it took per operation.  Just make sure you use doubles or you&rsquo;ll lose all of your precision due to coercion.  Do this (assuming that your number of operations is stored in a variable called &ldquo;operations&rdquo;):</p>

<pre><code>double millisecondsPerOperation = (double) durationInMilliseconds / (double) operations;
double operationsPerMillisecond = (double) operations / (double) durationInMilliseconds;
</code></pre>

<p>These are just reciprocals of each other but sometimes one value is a lot easier to understand than the other so I usually calculate them both.</p>

<p>Now that you have those statistics you can try different thread counts, optimize code/loops, etc.  Good luck!  Post in the comments with any ideas and/or issues.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How-To: Fix "Chown: Cannot Dereference" Errors in Cloudera CDH on Debian/Ubuntu Linux When Upgrading]]></title>
    <link href="http://blog.timmattison.com/archives/2012/02/26/how-to-fix-chown-cannot-dereference-errors-in-cloudera-cdh-on-debianubuntu-linux-when-upgrading/"/>
    <updated>2012-02-26T12:10:47-05:00</updated>
    <id>http://blog.timmattison.com/archives/2012/02/26/how-to-fix-chown-cannot-dereference-errors-in-cloudera-cdh-on-debianubuntu-linux-when-upgrading</id>
    <content type="html"><![CDATA[<p><strong><em>WARNING!</em></strong> Do not do this on production clusters unless you are willing to take responsibility for any issues that may occur.  This wipes out all of your logs and potentially other files.  Always have a backup before trying anything like this.  I take no responsibility for issues that may arise from running any or all of these instructions.</p>

<p>When I tried to upgrade my CDH installation today I received many errors from dpkg that caused the upgrade to fail.  The errors looked like this:</p>

<pre><code>chown: cannot dereference `/var/log/hadoop-0.20/userlogs/job_201202031049_0008/attempt_201202031049_0008_m_000015_0': No such file or directory
chown: cannot dereference `/var/log/hadoop-0.20/userlogs/job_201202031049_0008/attempt_201202031049_0008_m_000003_0': No such file or directory
chown: cannot dereference `/var/log/hadoop-0.20/userlogs/job_201202031049_0008/attempt_201202031049_0008_m_000009_0': No such file or directory
chown: cannot dereference `/var/log/hadoop-0.20/userlogs/job_201202031049_0008/attempt_201202031049_0008_m_000018_0': No such file or directory

...

dpkg: error processing hadoop-0.20 (--configure):
 subprocess installed post-installation script returned error exit status 123
dpkg: dependency problems prevent configuration of hadoop-0.20-tasktracker:
 hadoop-0.20-tasktracker depends on hadoop-0.20 (= 0.20.2+923.195-1~squeeze-cdh3); however:
  Package hadoop-0.20 is not configured yet.
</code></pre>

<p>My simple fix, not for production clusters, is to do the following:</p>

<ul>
<li><p>Step 1: Become the HDFS user and stop Hadoop by running</p>

<p>~/bin/stop-all.sh</p></li>
<li><p>Step 2: Become root and remove all of your Hadoop related logs by running</p>

<p>rm -rf /var/log/hadoop-0.20/*</p></li>
<li><p>Step 3: Become root and run your upgrade by running</p>

<p>apt-get upgrade</p></li>
<li><p>Step 4: Become the HDFS user and restart Hadoop by running</p>

<p>~/bin/start-all.sh</p></li>
</ul>


<p>After that your installation should be working and up to date again.  Post in the comments if it works for your or if you need any assistance.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tip: Trimming the Tops and Bottoms of Text Files With Head and Tail]]></title>
    <link href="http://blog.timmattison.com/archives/2012/02/16/tip-trimming-the-tops-and-bottoms-of-text-files-with-head-and-tail/"/>
    <updated>2012-02-16T09:05:21-05:00</updated>
    <id>http://blog.timmattison.com/archives/2012/02/16/tip-trimming-the-tops-and-bottoms-of-text-files-with-head-and-tail</id>
    <content type="html"><![CDATA[<p>Normally the head and tail applications on Linux are good for what their names imply.  head gives you the first few lines of a file, tail gives you last few lines of a file and even lets you watch the end of a file for changes.  This is great but what if you want to get an entire file <em>except</em> for the first few or last few lines?  It turns out that head and tail have options to do this and it&rsquo;s incredibly useful for trimming files without knowing exactly how many lines they contain.</p>

<p>I&rsquo;m writing this because I keep forgetting which one does what.  Here&rsquo;s how you can remember it and use it every day&hellip;</p>

<p>Tip #1: If you only want the end of a file use tail like this:</p>

<pre><code>tail -n +3 input.file &gt; output.file
</code></pre>

<p>An example file, like a PostgreSQL database dump, might look like this:</p>

<pre><code>column_a | column_b | column_c
---------+----------+---------
    1    |   bob    |  65000
    2    |   joe    |  80000
    3    |   jim    |  54000
(3 rows)
</code></pre>

<p>After running</p>

<pre><code>tail -n +3 input.file &gt; output.file
</code></pre>

<p>on this we&rsquo;ll end up with output that looks like this:</p>

<pre><code>    1    |   bob    |  65000
    2    |   joe    |  80000
    3    |   jim    |  54000
(3 rows)
</code></pre>

<p>The best way to remember this is that you want everything until the end of the file starting at the third line.</p>

<p>Tip #2: If you only want the beginning of a file use head like this:</p>

<pre><code>head -n -2 input.file &gt; output.file
</code></pre>

<p>Using the same example file we end up with:</p>

<pre><code>column_a | column_b | column_c
---------+----------+---------
    1    |   bob    |  65000
    2    |   joe    |  80000
    3    |   jim    |  54000
</code></pre>

<p>The best way to remember this is that you want everything from the beginning file excluding the last two lines.  Note, there is a blank line after &ldquo;(3 rows)&rdquo; and we want to remove that too.</p>

<p>Tip #3: If you need to trim from both side you can pipe like this:</p>

<pre><code>tail -n +3 input.file | head -n -2 &gt; output.file
</code></pre>

<p>Using the same example we end up:</p>

<pre><code>    1    |   bob    |  65000
    2    |   joe    |  80000
    3    |   jim    |  54000
</code></pre>

<p>This now translates to start at the third line and stop two lines from the end.  If you ever forget just come back here and re-read the examples.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tip: Fix Basic Build Issues When Using Maven]]></title>
    <link href="http://blog.timmattison.com/archives/2012/02/07/tip-fix-basic-build-issues-when-using-maven/"/>
    <updated>2012-02-07T13:33:46-05:00</updated>
    <id>http://blog.timmattison.com/archives/2012/02/07/tip-fix-basic-build-issues-when-using-maven</id>
    <content type="html"><![CDATA[<p>When trying to build Hadoop with Maven today I got this ugly error message:</p>

<pre><code>[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[ERROR] FATAL ERROR
[INFO] ------------------------------------------------------------------------
[INFO] Error building POM (may not be this project's POM).


Project ID: org.apache.hadoop:hadoop-project
POM Location: /home/tim/subversion/hadoop-trunk/hadoop-project/pom.xml
Validation Messages:

    [0]  For dependency Dependency {groupId=jdk.tools, artifactId=jdk.tools, version=1.6, type=jar}: system-scoped dependency must specify an absolute path systemPath.


Reason: Failed to validate POM for project org.apache.hadoop:hadoop-project at /home/tim/subversion/hadoop-trunk/hadoop-project/pom.xml


[INFO] ------------------------------------------------------------------------
[INFO] Trace
org.apache.maven.reactor.MavenExecutionException: Failed to validate POM for project org.apache.hadoop:hadoop-project at /home/tim/subversion/hadoop-trunk/hadoop-project/pom.xml
    at org.apache.maven.DefaultMaven.getProjects(DefaultMaven.java:404)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:272)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:138)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:362)
    at org.apache.maven.cli.compat.CompatibleMain.main(CompatibleMain.java:60)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:616)
    at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
    at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
    at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
    at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
Caused by: org.apache.maven.project.InvalidProjectModelException: Failed to validate POM for project org.apache.hadoop:hadoop-project at /home/tim/subversion/hadoop-trunk/hadoop-project/pom.xml
    at org.apache.maven.project.DefaultMavenProjectBuilder.processProjectLogic(DefaultMavenProjectBuilder.java:1077)
    at org.apache.maven.project.DefaultMavenProjectBuilder.buildInternal(DefaultMavenProjectBuilder.java:880)
    at org.apache.maven.project.DefaultMavenProjectBuilder.buildFromSourceFileInternal(DefaultMavenProjectBuilder.java:508)
    at org.apache.maven.project.DefaultMavenProjectBuilder.build(DefaultMavenProjectBuilder.java:200)
    at org.apache.maven.DefaultMaven.getProject(DefaultMaven.java:604)
    at org.apache.maven.DefaultMaven.collectProjects(DefaultMaven.java:487)
    at org.apache.maven.DefaultMaven.collectProjects(DefaultMaven.java:560)
    at org.apache.maven.DefaultMaven.getProjects(DefaultMaven.java:391)
    ... 12 more
[INFO] ------------------------------------------------------------------------
[INFO] Total time: &lt; 1 second
[INFO] Finished at: Tue Feb 07 13:30:39 EST 2012
[INFO] Final Memory: 3M/361M
[INFO] ------------------------------------------------------------------------
</code></pre>

<p>Tsk, tsk on me.  All I had to do was set my JAVA_HOME variable.  Not sure how to set yours?  Just do this:</p>

<pre><code>export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
</code></pre>

<p>I&rsquo;m actually running 64-bit Debian Wheezy so I had to do some other things to get my system prepped.  I needed to get off of Maven 2 and onto Maven 3, add the JDK, install the protocol buffers compiler, install zlib and its development files, and use a slightly different path.  Here&rsquo;s what I did:</p>

<pre><code>sudo apt-get install maven openjdk-6-jdk libprotoc-dev protobuf-compiler zlib1g-dev
export JAVA_HOME="/usr/lib/jvm/java-6-openjdk-amd64/
</code></pre>

<p>After that Maven started humming away when I ran:</p>

<pre><code>mvn compile -Pnative
</code></pre>

<p>Good luck!  Post in the comments if you run into trouble.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tip: Fix NoClassDefFoundError on org/apache/hadoop/thirdparty/guava/common/collect/LinkedListMultimap]]></title>
    <link href="http://blog.timmattison.com/archives/2012/02/07/tip-fix-noclassdeffounderror-on-orgapachehadoopthirdpartyguavacommoncollectlinkedlistmultimap/"/>
    <updated>2012-02-07T11:16:19-05:00</updated>
    <id>http://blog.timmattison.com/archives/2012/02/07/tip-fix-noclassdeffounderror-on-orgapachehadoopthirdpartyguavacommoncollectlinkedlistmultimap</id>
    <content type="html"><![CDATA[<p>If you&rsquo;re writing code that accesses HDFS and you get an exception that looks like this:</p>

<pre><code>Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/thirdparty/guava/common/collect/LinkedListMultimap
    at org.apache.hadoop.hdfs.SocketCache.&lt;init&gt;(SocketCache.java:48)
    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:240)
    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:208)
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1563)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:67)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:1597)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1579)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:228)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:111)
</code></pre>

<p>Make sure you include the guava-r09-jarjar.jar JAR in your build path.  This is usually located in /usr/lib/hadoop-0.20/lib.</p>
]]></content>
  </entry>
  
</feed>
