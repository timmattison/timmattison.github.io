
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Tim Mattison</title>
  <meta name="author" content="Tim Mattison">

  
  <meta name="description" content="If you&rsquo;re writing code that accesses HDFS and you get an exception that looks like this: Exception in thread "main" java.lang. &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.timmattison.com/page/6">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Tim Mattison" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-46746763-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Tim Mattison</a></h1>
  
    <h2>Hardcore tech</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:blog.timmattison.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Home</a></li>
  <li><a href="/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/02/07/tip-fix-noclassdeffounderror-on-orgapachehadoopthirdpartyguavacommoncollectlinkedlistmultimap/">Tip: Fix NoClassDefFoundError on org/apache/hadoop/thirdparty/guava/common/collect/LinkedListMultimap</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-02-07T11:16:19-05:00" pubdate data-updated="true">Feb 7<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/02/07/tip-fix-noclassdeffounderror-on-orgapachehadoopthirdpartyguavacommoncollectlinkedlistmultimap/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/02/07/tip-fix-noclassdeffounderror-on-orgapachehadoopthirdpartyguavacommoncollectlinkedlistmultimap/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>If you&rsquo;re writing code that accesses HDFS and you get an exception that looks like this:</p>

<pre><code>Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/thirdparty/guava/common/collect/LinkedListMultimap
    at org.apache.hadoop.hdfs.SocketCache.&lt;init&gt;(SocketCache.java:48)
    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:240)
    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:208)
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1563)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:67)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:1597)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1579)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:228)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:111)
</code></pre>

<p>Make sure you include the guava-r09-jarjar.jar JAR in your build path.  This is usually located in /usr/lib/hadoop-0.20/lib.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/02/07/how-to-debug-hdfs-applications-in-eclipse/">How-To: Debug HDFS Applications in Eclipse</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-02-07T10:56:58-05:00" pubdate data-updated="true">Feb 7<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/02/07/how-to-debug-hdfs-applications-in-eclipse/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/02/07/how-to-debug-hdfs-applications-in-eclipse/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I started using the HDFS API in Java recently in order to port some legacy applications over to HDFS.  One thing that I noticed is that when running the application via &ldquo;hadoop jar&rdquo; it properly accessed HDFS and stored its files there but if I ran it in the debugger the API calls succeeded but the files never showed up.</p>

<p>After a bit more investigation I saw that the HDFS API was unable to read my configuration files and find the NameNode so it defaulted to writing the files on the local file system instead.  This is nice behavior for debugging sometimes but can be dangerous if you&rsquo;re running an application that must put its files in HDFS like a mission critical application that doesn&rsquo;t fulfill its operational contract if data is lost.  In the case of an application like that accidentally writing to the local file system could be disastrous and expensive so it&rsquo;s good to know how to detect when this happens, and/or overcome it in a situation where you&rsquo;re trying to debug against your HDFS cluster.</p>

<p>Let&rsquo;s look at a simple code snippet that connects to HDFS that is just a cleaned up version of <a href="http://developer.yahoo.com/hadoop/tutorial/module2.html#programmatically">Yahoo&rsquo;s Hadoop tutorial</a>:</p>

<pre><code>import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

public class TestClass {
    private static final String theFilename = "timmattison.txt";
    private static final String message = "This is the message that gets put into the file";

    public static void main(String[] args) {
        try {
            // Get the configuration and connect to the file system
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(conf);

            // Create the path object for our output file
            Path filenamePath = new Path(theFilename);

            // Does it exist already?
            if (fs.exists(filenamePath)) {
                // Yes, remove it first
                fs.delete(filenamePath);
            }

            // Create the output file and write the data into it
            FSDataOutputStream out = fs.create(filenamePath);
            out.writeUTF(message);
            out.close();

            // Open the output file as an input file and read it
            FSDataInputStream in = fs.open(filenamePath);
            String messageIn = in.readUTF();

            // Print its contents and close the file
            System.out.print(messageIn);
            in.close();
        } catch (IOException ioe) {
            System.err.println("IOException during operation: "
                    + ioe.toString());
            System.exit(1);
        }
    }
}
</code></pre>

<p>If you run this code with &ldquo;hadoop jar&rdquo; you&rsquo;ll see that it creates the expected file (timmattison.txt) in the current user&rsquo;s default path in HDFS.  If you run this code with Eclipse either in Run or Debug mode you&rsquo;ll see that the file is not created in HDFS, it is created relative to where Eclipse starts the JVM for the new process.</p>

<p>We can tell where the HDFS library will attempt to write our files by very simply checking the type of the FileSystem object that is created by the call to <code>FileSystem.get(conf)</code>.  If that object&rsquo;s type is <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/LocalFileSystem.html">LocalFileSystem</a> we are not connecting to HDFS.  However if that object&rsquo;s type is <a href="http://hadoop.apache.org/hdfs/docs/current/api/org/apache/hadoop/hdfs/DistributedFileSystem.html">DistributedFileSystem</a> then you know that you&rsquo;re connected to a Hadoop cluster and writing to a real instance of HDFS.</p>

<p>In your code you can leverage this in a few ways.  First, if you always need to be sure you&rsquo;re writing to the cluster you can check the fs variable and see if it is an instance of LocalFileSystem.  If it is you can signal an error, e-mail an admin, etc.  Configuration changes in the field could cause this to happen so it is important to be aware of.  In general running programs through &ldquo;hadoop jar&rdquo; will make sure this doesn&rsquo;t happen but a little <a href="http://en.wikipedia.org/wiki/Defensive_programming">defensive programming</a> usually can&rsquo;t hurt.  Just consider what the cost of running your code against the wrong file system would be and trap this condition accordingly.</p>

<p>If you&rsquo;re interested in handling this automatically in your development environment I&rsquo;ve come up with a simple pattern that works for me.  In some instances such as running your code outside of Eclipse without &ldquo;hadoop jar&rdquo; this pattern could fail so only use it specifically for debugging in Eclipse.  Here&rsquo;s what I do:</p>

<pre><code>import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.LocalFileSystem;

public class TestClass {
    private static final String CORE_SITE_NAME = "core-site.xml";
    private static final String CORE_SITE_LOCATION = "/etc/hadoop-0.20/conf.empty/"
            + CORE_SITE_NAME;
    private static final String LOCAL_SEARCH_PATH = "bin/";
    private static final String LOCAL_CORE_SITE_LOCATION = LOCAL_SEARCH_PATH
            + CORE_SITE_NAME;

    private static boolean updatedConfiguration = false;

    public static void main(String[] args) throws IOException {
        try {
            // Get the configuration and connect to the file system
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(conf);

            // Is this the local file system?
            if (fs instanceof LocalFileSystem) {
                // Yes, we need to do use the cluster. Update the configuration.
                updatedConfiguration = true;

                /**
                 * Remove the file if it already exists. Just in case this is a
                 * symlink or something.
                 */
                removeTemporaryConfigurationFile();

                // Copy the core-site.xml file to where our JVM can see it
                copyConfigurationToTemporaryLocation();

                // Recreate the configuration object
                conf = new Configuration();

                // Get a new file system object
                fs = FileSystem.get(conf);

                // Is this the local file system?
                if (fs instanceof LocalFileSystem) {
                    // Yes, give up. We cannot connect to the cluster.
                    System.err.println("Failed to connect to the cluster.");
                    System.exit(2);
                }
            }

            // Do your HDFS related work here...
        } catch (IOException ioe) {
            // An IOException occurred, give up
            System.err.println("IOException during operation: "
                    + ioe.toString());
            System.exit(1);
        } finally {
            // Did we update the configuration?
            if (updatedConfiguration) {
                // Yes, clean up the temporary configuration file
                removeTemporaryConfigurationFile();
            }
        }
    }

    private static void copyConfigurationToTemporaryLocation()
            throws IOException {
        Runtime.getRuntime().exec(
                new String[] { "cp", CORE_SITE_LOCATION,
                        LOCAL_CORE_SITE_LOCATION });
    }

    private static void removeTemporaryConfigurationFile() throws IOException {
        Runtime.getRuntime().exec(
                new String[] { "rm", LOCAL_CORE_SITE_LOCATION });
    }
}
</code></pre>

<p>Now where it says &ldquo;Do your HDFS related work here&hellip;&rdquo; you can put your code and be sure that it&rsquo;s accessing the cluster, not your local file system.</p>

<p>In a future article and on github I&rsquo;ll wrap this up in a reusable chunk so that you won&rsquo;t have to copy and paste this every time you start a new project.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/29/why-i-wrote-dirhash-pl-a-whole-directory-filename-and-contents-hashing-system/">Why I Wrote dirhash.pl (a Whole Directory Filename and Contents Hashing System)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-29T17:17:15-05:00" pubdate data-updated="true">Jan 29<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/29/why-i-wrote-dirhash-pl-a-whole-directory-filename-and-contents-hashing-system/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/29/why-i-wrote-dirhash-pl-a-whole-directory-filename-and-contents-hashing-system/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Over the years I have accumulated many, many disks full of data.  Ordinarily I migrate them to my latest disk and just keep a backup or two and get rid of the old disks.  However, every once in a while I come across an old disk that has a directory structure that I really want to keep but don&rsquo;t have the time to go through.  Recently this happened with a large directory of old projects and to make matters worse there were multiple copies of this directory in different places.</p>

<p>I looked around and found some interesting stuff so I certainly didn&rsquo;t want to lose this time capsule of code but I now had an interesting problem.  How do I take multiple copies of what appear like identical directories and compare them so I can nuke them if they are duplicates?</p>

<p>I could rsync one to the other but that would involve modifying one or both copies and I wasn&rsquo;t excited about that since there&rsquo;s the chance that one is newer than the other and I could mix them up.  There are flags to check that but I didn&rsquo;t want to go the potentially destructive route.</p>

<p>I could hash each file and determine which are dupes but with over 20,000 files it becomes tedious work.  Dupe removers are OK but it still felt clunky.  I just wanted a quick yea or nay as to whether I could give these files the good old &ldquo;rm -rf&rdquo;.</p>

<p>Hashing the files, dumping it to a list, and then hashing the list sounds good but there are a few problems:</p>

<ul>
<li><p>If the files are in different order the results will be different but I can resolve this with &ldquo;sort&rdquo;</p></li>
<li><p>If the hashing program reports the relative path names then the file list will definitely be different and therefore the final hash will be different.  I could resolve this with some vi-fu or sed-fu but that leaves me with some work that is just lost when I need to do it again.</p></li>
</ul>


<p>My solution was to write a Perl script that rolled all of that logic into one package.  It does the following:</p>

<ul>
<li><p>Gets a list of files in the specified directory</p></li>
<li><p>Sorts that list</p></li>
<li><p>Hashes each entry in the list.  If it is a file it is hashed and its filename is appended to the hash, if it is a directory it is opened up and those files are run through this process recursively.</p></li>
<li><p>The final list of hashes and filenames itself is hashed to make sure we get one hash for the entire directory</p></li>
</ul>


<p>This solves the order problem and the script removes the relative path elements from each filename automatically so that&rsquo;s no longer an issue as well.</p>

<p>If you&rsquo;re looking to see if two directory structures are equal (filenames all equal, no files added or removed between the two of them, and file contents equal) give it a shot and let me know how it works for you.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/29/why-i-wrote-prcp-pl-copies-files-with-a-progress-indicator/">Why I Wrote prcp.pl (Copies Files With a Progress Indicator)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-29T17:01:40-05:00" pubdate data-updated="true">Jan 29<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/29/why-i-wrote-prcp-pl-copies-files-with-a-progress-indicator/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/29/why-i-wrote-prcp-pl-copies-files-with-a-progress-indicator/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Just last night I had a few large files I wanted to bring to a friends house.  They were HD videos that I took a long time ago and were about 7 GB in total.  I figured I would just plop them on a USB drive and in a few minutes I&rsquo;d be ready to go.</p>

<p>It didn&rsquo;t quite work out so easily.  15 minutes after I tried to drag the files to the drive in Nautilus it wasn&rsquo;t apparent that anything was happening.  The progress bar had frozen, the flash drive was still doing something, and some of the files looked like they were there.  I tried to do it again on the command line using &ldquo;cp&rdquo; and got the same results (with no progress bar).  I knew there had to be a better way to copy these files and know what was actually going on but first I had to figure out what was really happening.</p>

<p>The initial discovery was that when &ldquo;cp&rdquo; copies a file to a vfat (FAT32) formatted flash drive it immediately allocates enough space for the entire file to fit.  This is smart because it makes sure you can&rsquo;t get stuck copying a file to a drive where it won&rsquo;t fit.  It&rsquo;s also a pain because I can&rsquo;t open another terminal window and see the file growing as it is being written to.</p>

<p>My next discovery was that disk caching was making the process difficult to understand.  Nautilus appeared to copy the file quickly at first and then it just looked like it hung completely.</p>

<p>So I set out to write a script to fix these problems.  I knew I wanted all of the following things:</p>

<ul>
<li><p>A command-line utility &ndash; I wanted to be able to use this program without a GUI</p></li>
<li><p>A progress bar &ndash; I wanted to get visual feedback without babysitting the script</p></li>
<li><p>Files that &ldquo;grew&rdquo; while they were being copied &ndash; I wanted to be able to check it remotely in a different terminal session if necessary</p></li>
<li><p>Predictable behavior with regard to the disk cache &ndash; I couldn&rsquo;t have the disk cache making it seem like the devices were writing very fast, then stalling, then writing very fast again</p></li>
</ul>


<p>After an hour I came up with prcp.pl.  It is a Perl script that uses mainly Term::ProgressBar and File::Sync to do what I needed.  It&rsquo;s not finished but it&rsquo;s very functional.  What it does is copy the file using sysread and syswrite while fsyncing the output file after each write.  This forces what you see on the disk to be consistent with what the program is copying since it largely bypasses the disk cache.</p>

<p>If you&rsquo;re brave give it a try and let me know what you think.  Only use it on data that you have backed up since it has undergone minimal testing so far.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/29/free-tool-collection-available-on-github/">Free Tool Collection Available on Github</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-29T16:38:52-05:00" pubdate data-updated="true">Jan 29<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/29/free-tool-collection-available-on-github/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/29/free-tool-collection-available-on-github/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Today I just released <a href="https://github.com/timmattison/timmattison-tools">a few home brew tools</a> that I&rsquo;ve been meaning to write for a while.  I&rsquo;m going to write full articles on them soon but just as a preliminary taste here are descriptions of the two programs I released today:</p>

<ul>
<li><p>prcp.pl &ndash; Copies files and has a progress indicator</p></li>
<li><p>dirhash.pl &ndash; Hashes the names and contents of an entire directory to a single hash</p></li>
</ul>


<p>If you use them please post in the comments below.  If you want to contribute to them just send me merge requests and open tickets!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/24/mini-hack-parallel-vacuuming-in-postgresql/">Mini Hack: Parallel Vacuuming in PostgreSQL</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-24T11:42:41-05:00" pubdate data-updated="true">Jan 24<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/24/mini-hack-parallel-vacuuming-in-postgresql/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/24/mini-hack-parallel-vacuuming-in-postgresql/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I run several development environments that I need to sync with production databases to do bug fixes and new feature development.  It&rsquo;s always good to vacuum these databases before using them especially if you&rsquo;re doing it via rsync on a live system.  If you don&rsquo;t you could end up with rows that are inaccessible from your indicies and get strange results from your database.</p>

<p>Full vacuums are slow but we can&rsquo;t get around it here.  What I noticed is that normally in production vacuuming the database is an I/O bound operation but in development where we&rsquo;re working with dedicated development machines with tons of RAM we typically end up with a lot of the database for our smaller projects (&lt; 5 GB) in the cache.  This makes vacuuming a CPU bound process again and where there&rsquo;s a CPU bound process there&rsquo;s usually room for parallelism.</p>

<p>Today I decided to test out how well <a href="http://www.gnu.org/software/parallel/">GNU Parallel</a> could speed up my development machine&rsquo;s vacuuming process and I&rsquo;m happy to report that it cuts it nearly in half.  If you&rsquo;re running into the same issue and waiting for vacuuming is eating into your development time try this one liner (make sure you have <a href="http://www.gnu.org/software/parallel/">GNU Parallel</a> installed first):</p>

<pre><code>echo "\dt" | psql DB_NAME | head -n -2 | tail -n +4 | awk ' { print $3 } ' | parallel -I {} vacuumdb -f -v -d DB_NAME -t {}
</code></pre>

<p>What this does is:</p>

<ul>
<li><p>Sends the string &ldquo;\dt&rdquo; to PostgreSQL to list the table names</p></li>
<li><p>Pipes that through head and removes the last two lines (they don&rsquo;t contain table names)</p></li>
<li><p>Pipes that through tail and removes the first four lines (they also don&rsquo;t contain table names)</p></li>
<li><p>Pipes that to awk and extracts the third field (the table name)</p></li>
<li><p>Pipes that to parallel, runs vacuumdb with a full vacuum (-f) in verbose mode (-v), placing the table names where we included the curly bracket pair</p></li>
</ul>


<p>UPDATE: Ole Tange, the author of the fantastic GNU Parallel package, wrote in with his own one-liner to do the same thing as mine.  His is a bit shorter and requires fewer pipes.  Take a look:</p>

<pre><code>sql -n --list-tables pg:///DB_NAME | parallel -j0 -r --colsep '\|' sql pg:///DB_NAME vacuum full verbose {2}
</code></pre>

<p>What his does is:</p>

<ul>
<li><p>Gets a table list from GNU sql (which I had never used before, it&rsquo;s great to know that it exists!)</p></li>
<li><p>Pipes that to GNU Parallel specifying it should run as many jobs as the machine has cores (-j0), should not run if there is no input (-r), and uses the pipe character as a column separator</p></li>
<li><p>GNU Parallel then calls GNU sql, connects to the proper database executes a full, verbose vacuum on the second field it extracted from the table list (the table name)</p></li>
</ul>


<p>I added in the &ldquo;full verbose&rdquo; to Ole&rsquo;s example so the two scripts are doing the same work instead of just a plain vacuum.</p>

<p>Compare that against the run time for a normal vacuum and report your results in the comments.  For databases that won&rsquo;t fit in your RAM it may not help that much but I&rsquo;d like to hear either way.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/16/how-to-set-up-multi-head-x-in-debian-using-nvidia-cards/">How-To: Set Up Multi-head X in Debian Using Nvidia Cards</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-16T12:53:52-05:00" pubdate data-updated="true">Jan 16<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/16/how-to-set-up-multi-head-x-in-debian-using-nvidia-cards/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/16/how-to-set-up-multi-head-x-in-debian-using-nvidia-cards/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I just had to set up a multi-head system in Debian and it consumed way too much of my time.  I did see a lot of articles about it online but they all had a few common threads.</p>

<p>First, they all mentioned Xinerama was deprecated which initially scared me off of it.  Xinerama is deprecated but there&rsquo;s no replacement so for now I&rsquo;m using it until it&rsquo;s replacement is mature.  Wayland is a system that keeps getting mentioned but seems too far off to be considered for me.</p>

<p>Second, they didn&rsquo;t clearly outline how they determined all of their display information.  Some were better than others but I need to build a real list of commands to get things done properly and repeatably since I know I will have to do this again at some point.</p>

<p>So, let&rsquo;s go over what you need to do and how I got to a working system with three side-by-side displays.</p>

<p>My system consists of two Nvidia GTX 470 cards.  Each card has two DVI outputs but I only have three displays.  Therefore one card has two displays connected to it (the left and right displays) and another card has one display connected to it (the center display).</p>

<p>What you&rsquo;ll want to do is first figure out which display is connected to which card and how Linux enumerates them in your system.  To do this you should be in text mode (not X!) and go to the &ldquo;/sys/class/drm&rdquo; directory.  Let&rsquo;s take a look at mine:</p>

<pre><code>tim@desktop:/sys/class/drm$ ls -la
total 0
drwxr-xr-x  2 root root    0 Jan 15 19:21 .
drwxr-xr-x 40 root root    0 Jan 15 19:21 ..
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card0 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card0-DVI-I-1 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0/card0-DVI-I-1
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card0-DVI-I-2 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0/card0-DVI-I-2
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card0-HDMI-A-1 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0/card0-HDMI-A-1
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card1 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card1-DVI-I-3 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1/card1-DVI-I-3
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card1-DVI-I-4 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1/card1-DVI-I-4
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card1-HDMI-A-2 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1/card1-HDMI-A-2
lrwxrwxrwx  1 root root    0 Jan 15 19:21 controlD64 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/controlD64
lrwxrwxrwx  1 root root    0 Jan 15 19:21 controlD65 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/controlD65
lrwxrwxrwx  1 root root    0 Jan 15 19:21 ttm -&gt; ../../devices/virtual/drm/ttm
-r--r--r--  1 root root 4096 Jan 15 19:21 version
</code></pre>

<p>Here you can see that I have two cards named card0 and card1.  Card 0 has three connectors on it named card0-DVI-I-1, card0-DVI-2, and card0-HDMI-A-1.  Card 1 has three connectors on it named card1-DVI-3, card1-DVI-4, card1-HDMI-A-2.  We will need this information later to build our xorg.conf so keep it handy.</p>

<p>Now you&rsquo;ll want to determine which monitor is hooked up to which card and port.  If you are using this setup in Windows already you are not guaranteed that the cards are recognized in the same order so just forget what Windows has told you.  To figure out which is which first make sure all of your displays are connected to your cards.  Then ask Linux which displays are connected by executing this command:</p>

<pre><code>ls /sys/class/drm/*/status | xargs -I {} -i bash -c "echo -n {}: ; cat {}"
</code></pre>

<p>This will show you which connectors think that they have something connected to them.  My output looks like this:</p>

<pre><code>tim@desktop:/sys/class/drm$ ls /sys/class/drm/*/status | xargs -I {} -i bash -c "echo -n {}: ; cat {}"
/sys/class/drm/card0-DVI-I-1/status:connected
/sys/class/drm/card0-DVI-I-2/status:connected
/sys/class/drm/card0-HDMI-A-1/status:disconnected
/sys/class/drm/card1-DVI-I-3/status:connected
/sys/class/drm/card1-DVI-I-4/status:disconnected
/sys/class/drm/card1-HDMI-A-2/status:disconnected
</code></pre>

<p>So now I know that card0 is the card that has my two displays attached to it and card1 is the card that has my two displays attached to it.  That tells me that &ldquo;DVI-I-3&rdquo; is my center display and &ldquo;DVI-I-1&rdquo; and &ldquo;DVI-I-2&rdquo; are my left and right displays.  We&rsquo;ll figure out which is left and which is right in a minute.</p>

<p>For now, let&rsquo;s figure out which PCI device Linux considers these devices.  You can do this by running this command:</p>

<pre><code>ls -la /sys/class/drm/card?
</code></pre>

<p>My output looks like this:</p>

<pre><code>tim@desktop:~$ ls -la /sys/class/drm/card?
lrwxrwxrwx 1 root root 0 Jan 15 19:21 /sys/class/drm/card0 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0
lrwxrwxrwx 1 root root 0 Jan 15 19:21 /sys/class/drm/card1 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1
</code></pre>

<p>Look at the numbers directly before &ldquo;drm&rdquo;.  They are &ldquo;0000:01:00.0&rdquo; for card 0 and &ldquo;0000:02:00.0&rdquo; for card 1.  This tells us that card 0 is PCI device 1 and card 1 is PCI device 2.  Keep these numbers handy too.</p>

<p>The next step is to figure out which connectors is the left and which is the right.  You can do this by guessing but let&rsquo;s just get a definitive answer.  Type the same command we used before to get the list of all of our displays (or press the up arrow if your shell supports that to get to the last command) but don&rsquo;t run it yet.  Now disconnect one display, press enter, and reconnect it.  I suggest you do this so that just in case you disconnect the display that your terminal is running on you can still run the command and see the results when you reconnect it.</p>

<p>In my case I got this output when my right display was disconnected:</p>

<pre><code>tim@desktop:/sys/class/drm$ ls /sys/class/drm/*/status | xargs -I {} -i bash -c "echo -n {}: ; cat {}"
/sys/class/drm/card0-DVI-I-1/status:disconnected
/sys/class/drm/card0-DVI-I-2/status:connected
/sys/class/drm/card0-HDMI-A-1/status:disconnected
/sys/class/drm/card1-DVI-I-3/status:connected
/sys/class/drm/card1-DVI-I-4/status:disconnected
/sys/class/drm/card1-HDMI-A-2/status:disconnected
</code></pre>

<p>Now I know that &ldquo;DVI-I-1&rdquo; is my right display since before it was connected and when I ran the command it was disconnected.  By elimination I know &ldquo;DVI-I-2&rdquo; is my right display.  If you have more displays you&rsquo;ll need to do this a few more times.</p>

<p>Let&rsquo;s recap.  I know that card 0 has two displays on it, its DVI-I-1 connector is my right display, its DVI-I-2 connector is my left display.  Card 1 has one display on it and its DVI-I-3 connector is my center display.  With that information I can start building my xorg.conf file.  I&rsquo;ll show you what I ended up with and then break it out and explain it:</p>

<pre><code>Section "ServerLayout"
    Identifier  "X.org Configured"
    Option      "Xinerama"            "on"
    Screen      0  "Screen0" 0 0
    Screen      1  "Screen1" RightOf  "Screen0"
    Screen      2  "Screen2" LeftOf   "Screen0"
    InputDevice    "Mouse0"           "CorePointer"
    InputDevice    "Keyboard0"        "CoreKeyboard"
EndSection

Section "Files"
    ModulePath   "/usr/lib/xorg/modules"
    FontPath     "/usr/share/fonts/X11/misc"
    FontPath     "/usr/share/fonts/X11/cyrillic"
    FontPath     "/usr/share/fonts/X11/100dpi/:unscaled"
    FontPath     "/usr/share/fonts/X11/75dpi/:unscaled"
    FontPath     "/usr/share/fonts/X11/Type1"
    FontPath     "/usr/share/fonts/X11/100dpi"
    FontPath     "/usr/share/fonts/X11/75dpi"
    FontPath     "/var/lib/defoma/x-ttcidfont-conf.d/dirs/TrueType"
    FontPath     "built-ins"
EndSection

Section "Module"
    Load  "glx"
    Load  "dbe"
    Load  "dri"
    Load  "record"
    Load  "dri2"
    Load  "extmod"
EndSection

Section "InputDevice"
    Identifier  "Keyboard0"
    Driver      "kbd"
EndSection

Section "InputDevice"
    Identifier  "Mouse0"
    Driver      "mouse"
    Option      "Protocol" "auto"
    Option      "Device" "/dev/input/mice"
    Option      "ZAxisMapping" "4 5 6 7"
EndSection

Section "Device"
    Identifier  "Device1"
    Driver      "nouveau"
    BusID       "PCI:1:0:0"
    Option      "ZaphodHeads" "DVI-I-1"
    Screen      0
EndSection

Section "Device"
    Identifier  "Device2"
    Driver      "nouveau"
    BusID       "PCI:1:0:0"
    Option      "ZaphodHeads" "DVI-I-2"
    Screen      1
EndSection

Section "Device"
    Identifier  "Device0"
    Driver      "nouveau"
    BusID       "PCI:2:0:0"
    Option      "ZaphodHeads" "DVI-I-3"
    Screen      0
EndSection

Section "Screen"
    Identifier  "Screen0"
    Device      "Device0"
EndSection

Section "Screen"
    Identifier  "Screen1"
    Device      "Device1"
EndSection

Section "Screen"
    Identifier  "Screen2"
    Device      "Device2"
EndSection
</code></pre>

<p>Here&rsquo;s the explanation of each section:</p>

<pre><code>Section "ServerLayout"
    Identifier  "X.org Configured"
    Option      "Xinerama"            "on"
    Screen      0  "Screen0" 0 0
    Screen      1  "Screen1" RightOf  "Screen0"
    Screen      2  "Screen2" LeftOf   "Screen0"
    InputDevice    "Mouse0"           "CorePointer"
    InputDevice    "Keyboard0"        "CoreKeyboard"
EndSection
</code></pre>

<p>This is the server layout section that declares how the screens are laid out and what input devices we&rsquo;re using.  It specifies a name of &ldquo;X.org Configured&rdquo; since I used another configuration as a baseline for this.  You can change that name to whatever you&rsquo;d like.  Then it specifies that Xinerama is on.  This means that you can drag windows back and forth between screens.  It has some downsides but they are outweighed by the convenience of placing windows anywhere across all of your displays.</p>

<p>Then it specifies three &ldquo;screen&rdquo; entries.  Later on &ldquo;Screen0&rdquo; will by my center screen.  &ldquo;0 0&rdquo; just means that that screen starts at pixel 0,0.  &ldquo;Screen1&rdquo; will be might right screen so it will be to the right of my center screen.  In xorg.conf language that is done with &ldquo;RightOf&rdquo;.  &ldquo;Screen2&rdquo; will be my left screen so it is &ldquo;LeftOf&rdquo; my center screen.</p>

<pre><code>Section "Files"
    ModulePath   "/usr/lib/xorg/modules"
    FontPath     "/usr/share/fonts/X11/misc"
    FontPath     "/usr/share/fonts/X11/cyrillic"
    FontPath     "/usr/share/fonts/X11/100dpi/:unscaled"
    FontPath     "/usr/share/fonts/X11/75dpi/:unscaled"
    FontPath     "/usr/share/fonts/X11/Type1"
    FontPath     "/usr/share/fonts/X11/100dpi"
    FontPath     "/usr/share/fonts/X11/75dpi"
    FontPath     "/var/lib/defoma/x-ttcidfont-conf.d/dirs/TrueType"
    FontPath     "built-ins"
EndSection

Section "Module"
    Load  "glx"
    Load  "dbe"
    Load  "dri"
    Load  "record"
    Load  "dri2"
    Load  "extmod"
EndSection
</code></pre>

<p>I left these two sections completely stock.  They&rsquo;re magic for now so don&rsquo;t make any changes to them.</p>

<pre><code>Section "InputDevice"
    Identifier  "Keyboard0"
    Driver      "kbd"
EndSection

Section "InputDevice"
    Identifier  "Mouse0"
    Driver      "mouse"
    Option      "Protocol" "auto"
    Option      "Device" "/dev/input/mice"
    Option      "ZAxisMapping" "4 5 6 7"
EndSection
</code></pre>

<p>These two sections specify our input devices.  These are stock also so you can leave them alone too.</p>

<pre><code>Section "Device"
    Identifier  "Device1"
    Driver      "nouveau"
    BusID       "PCI:1:0:0"
    Option      "ZaphodHeads" "DVI-I-1"
    Screen      0
EndSection
</code></pre>

<p>Here&rsquo;s some more configuration we have to do.  Remember that DVI-I-1 is our right display and we are calling it &ldquo;Screen1&rdquo;.  In this section we need to create a device for it that tells X which drive, card, and connector it uses as well as which screen it is relative to its specific graphics card.  We&rsquo;ll call it &ldquo;Device1&rdquo;, use the open-source Nvidia driver called &ldquo;Nouveau&rdquo;, tell it to use PCI device 1 (since card 0 was device 1) with the &ldquo;PCI:1:0:0&rdquo; string, then tell it the connector is &ldquo;DVI-I-1&rdquo; with the &ldquo;ZaphodHeads&rdquo; option, and finally that this is the first display (display 0) on this graphics card.</p>

<pre><code>Section "Device"
    Identifier  "Device2"
    Driver      "nouveau"
    BusID       "PCI:1:0:0"
    Option      "ZaphodHeads" "DVI-I-2"
    Screen      1
EndSection
</code></pre>

<p>For our left display we have similar settings but the connector is &ldquo;DVI-I-2&rdquo; and this is the second display (display 1) on this graphics card.  We also call this &ldquo;Device2&rdquo;.</p>

<pre><code>Section "Device"
    Identifier  "Device0"
    Driver      "nouveau"
    BusID       "PCI:2:0:0"
    Option      "ZaphodHeads" "DVI-I-3"
    Screen      0
EndSection
</code></pre>

<p>Our center screen is &ldquo;Device0&rdquo; and it is on a different graphics card.  Remember card 1 was PCI device 2 so we use &ldquo;PCI:2:0:0&rdquo; here and specify the connector as &ldquo;DVI-I-3&rdquo;.  This is the first display (display 0) on this graphics card.</p>

<pre><code>Section "Screen"
    Identifier  "Screen0"
    Device      "Device0"
EndSection

Section "Screen"
    Identifier  "Screen1"
    Device      "Device1"
EndSection

Section "Screen"
    Identifier  "Screen2"
    Device      "Device2"
EndSection
</code></pre>

<p>Finally, these sections map our devices to name screens.  In this case DeviceX is ScreenX.  Nothing fancy goes on here.</p>

<p>Replace or create your xorg.conf using the steps you see here and you should be up and running in no time.  I had to change from Gnome to Xfce since Gnome had a really tall black bar that took over my center display that I couldn&rsquo;t get rid of.  If you run into the same problem try Xfce or another window manager instead.</p>

<p>Good luck and post in the comments if you found this useful or need some assistance.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/10/how-to-find-and-fix-duplicate-java-object-serial-numbers/">How-To: Find and Fix Duplicate Java Object Serial Numbers</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-10T09:43:03-05:00" pubdate data-updated="true">Jan 10<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/10/how-to-find-and-fix-duplicate-java-object-serial-numbers/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/10/how-to-find-and-fix-duplicate-java-object-serial-numbers/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In Java making your objects serializable is often required to work with certain libraries (<a href="http://en.wikipedia.org/wiki/Object-relational_mapping">ORM</a>, <a href="http://code.google.com/webtoolkit/doc/latest/tutorial/RPC.html">GWT-RPC</a>, etc).  It&rsquo;s tempting when Eclipse tells us that our object is missing a unique identifier to have it generate one on the fly.  However, in practice what happens sometimes in the field is that you&rsquo;ll create a serializable class and then copy it to make a new class.  When you do this you&rsquo;re inadvertently copying the objects &ldquo;serialVersionUID&rdquo; field.</p>

<p>Why is this an issue?  Well, the serialVersionUID is supposed to be a universal identifier.  If it&rsquo;s not universal then you&rsquo;re asking for trouble.  When you deserialize a new version of this object or try to deserialize an object to the wrong object type you can end up with strange results or have your application crash altogether.</p>

<p>How do we fix it?  I&rsquo;ve written a command-line one liner (even though it looks like three lines on the site) that will print out a list of the files that have duplicated serialVersionUID values.  Here it is:</p>

<pre><code>grep -r serialVersionUID * | grep -o "private.*" | sort | uniq -d | sed 's/^.*=//' | sed 's/;//' | sed 's/^ //' | sed 's/^-/\\\\-/' | xargs -I PATTERNS grep -r PATTERNS . | cut -f1 -d ':'
</code></pre>

<p>Go into each of these files, remove their serialVersionUID field, and regenerate them.  Don&rsquo;t concern yourself with which ones overlap unless you already have serialized versions of these objects lying around.  If you do have serialized versions you should remove the field and reserialize your objects into your data store without it.  Then you can try to generate new values and again reserialize them into the data store.  DO NOT do this in production, always do it in a test environment to make sure that nothing breaks.  You do have unit tests that&rsquo;ll test these things, right?  :)</p>

<p>Post in the comments if you find this useful and keep hacking!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/09/mini-hack-approximating-the-size-of-a-java-object/">Mini Hack: Approximating the Size of a Java Object</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-09T16:31:08-05:00" pubdate data-updated="true">Jan 9<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/09/mini-hack-approximating-the-size-of-a-java-object/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/09/mini-hack-approximating-the-size-of-a-java-object/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Finding the size of an object or data structure used to be simple in &ldquo;the good old days&rdquo;.  You could just use the sizeof function and it&rsquo;d spit out something that usually made sense.  In Java there&rsquo;s nothing that seems to do it quite that simply.  I&rsquo;ve seen some solutions that involve using Java instrumentation but they have interesting quirks that can confuse things considerably.  For example, using it to obtain the size of a String object will always show 32 bytes as I found out by reading a thread on StackOverflow.</p>

<p>In any case I wanted something simple that would do the job.  My requirements were that there was minimal or zero setup, results were fairly close to the actual memory footprint, and that it work consistently unless I was using some kind of very strange data structure.  Here&rsquo;s what I came up with:</p>

<pre><code>try {
  // Get a ByteArrayOutputStream to catch the output of the
  //   ObjectOutputStream without going to disk
  ByteArrayOutputStream baos = new ByteArrayOutputStream();

  // Get an ObjectOutputStream so we can dump the entire
  //   object at one shot
  ObjectOutputStream oos = new ObjectOutputStream(baos);

  // Write the object
  oos.writeObject(o);

  // Close the stream
  oos.close();

  // Query the ByteArrayOutputStream for its size
  return baos.size();
} catch (IOException e) {
  // Something went wrong.  Print the stack trace.
  e.printStackTrace();

  // Return -1 so the caller knows we failed
  return -1;
}
</code></pre>

<p>Put this into your standard utility class as a static method and call it whenever you&rsquo;re curious about memory usage.  The output of this should be a little higher than the actual in-memory usage so you can consider this a high water mark for your object&rsquo;s size.</p>

<p>Post in the comments if you end up using it!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2011/12/26/how-to-disable-hdfs-permissions-for-hadoop-development/">How-To: Disable HDFS Permissions for Hadoop Development</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-12-26T11:47:35-05:00" pubdate data-updated="true">Dec 26<span>th</span>, 2011</time>
        
           | <a href="/archives/2011/12/26/how-to-disable-hdfs-permissions-for-hadoop-development/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2011/12/26/how-to-disable-hdfs-permissions-for-hadoop-development/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>If you&rsquo;ve <a href="http://blog.timmattison.com/archives/2011/12/23/how-to-install-hadoop-on-debian-ubuntu/">set up Hadoop for development</a> you may be wondering why you can&rsquo;t read or write files or create MapReduce jobs then you&rsquo;re probably missing a tiny bit of configuration.  For most development systems in pseudo-distributed mode it&rsquo;s easiest to disable permissions altogether.  This means that any user, not just the &ldquo;hdfs&rdquo; user, can do anything they want to HDFS so do not do this in production unless you have a very good reason.</p>

<p>The error message you&rsquo;re most likely seeing if permissions are the problem is similar to this:</p>

<p><code>
put: org.apache.hadoop.security.AccessControlException: Permission denied: user=tim, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x
</code></p>

<p>If that&rsquo;s the case and you really want to disable permissions just add this snippet into your hdfs-site.xml file (located in /etc/hadoop-0.20/conf.empty/hdfs-site.xml on Debian Squeeze) in the configuration section:</p>

<pre><code>  &lt;property&gt;
    &lt;name&gt;dfs.permissions&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
</code></pre>

<p>Then restart Hadoop (su to the &ldquo;hdfs&rdquo; user and run bin/stop-all.sh then bin/start-all.sh) and try putting a file again.  You should now be able to read/write with no restrictions.</p>

<p>Good luck!  Post in the comments if it doesn&rsquo;t work for you.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/page/7/">&larr; Older</a>
    
    <a href="/archives">Archives</a>
    
    <a class="next" href="/page/5/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
	<h1>Like this site/article?  Donate with Bitcoin!</h1>
	<a href="bitcoin:1PcYJnuZPZFWUMUHzzBGHHshEbrVzYpY9Y"><img src="http://blockchain.info/qr?data=1PcYJnuZPZFWUMUHzzBGHHshEbrVzYpY9Y&size=200" alt="Bitcoin"></a>
	<a href="bitcoin:1PcYJnuZPZFWUMUHzzBGHHshEbrVzYpY9Y">1PcYJnuZPZFWUMUHzzBGHHshEbrVzYpY9Y</a>
</section>
<section>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Sidebar -->
<ins class="adsbygoogle"
     style="display:inline-block;width:120px;height:240px"
     data-ad-client="ca-pub-9307090849713032"
     data-ad-slot="1616086903"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/archives/2014/09/17/hacking-together-a-super-simple-webserver-with-netcat-on-a-raspberry-pi/">Hacking Together a Super Simple Webserver With Netcat on a Raspberry Pi</a>
      </li>
    
      <li class="post">
        <a href="/archives/2014/09/12/the-first-bitcoin-transaction/">The First Bitcoin Transaction</a>
      </li>
    
      <li class="post">
        <a href="/archives/2014/09/04/rid-yourself-of-smart-quotes-smart-dashes-and-automatic-spelling-correction-on-mac-os/">Rid Yourself of Smart Quotes, Smart Dashes, and Automatic Spelling Correction on Mac OS</a>
      </li>
    
      <li class="post">
        <a href="/archives/2014/09/04/using-ssh-agent-to-simplify-connecting-to-ec2/">Using SSH Agent to Simplify Connecting to EC2</a>
      </li>
    
      <li class="post">
        <a href="/archives/2014/09/02/full-example-code-showing-how-to-use-guice-and-jetty/">Full Example Code Showing How to Use Guice and Jetty</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/timmattison">@timmattison</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'timmattison',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/timmattison?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Tim Mattison -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'timmattison';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>





  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
