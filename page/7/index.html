
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Tim Mattison</title>
  <meta name="author" content="Tim Mattison">

  
  <meta name="description" content="Today I just released a few home brew tools that I&rsquo;ve been meaning to write for a while. I&rsquo;m going to write full articles on them soon &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.timmattison.com/page/7">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Tim Mattison" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">


<script type="text/javascript">
/* <![CDATA[ */
    (function() {
        var s = document.createElement('script'), t = document.getElementsByTagName('script')[0];
        s.type = 'text/javascript';
        s.async = true;
        s.src = 'http://api.flattr.com/js/0.6/load.js?mode=auto';
        t.parentNode.insertBefore(s, t);
    })();
/* ]]> */
</script>


  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-46746763-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Tim Mattison</a></h1>
  
    <h2>Hardcore tech</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:blog.timmattison.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Home</a></li>
  <li><a href="/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/29/free-tool-collection-available-on-github/">Free Tool Collection Available on Github</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-29T16:38:52-05:00" pubdate data-updated="true">Jan 29<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/29/free-tool-collection-available-on-github/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/29/free-tool-collection-available-on-github/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Today I just released <a href="https://github.com/timmattison/timmattison-tools">a few home brew tools</a> that I&rsquo;ve been meaning to write for a while.  I&rsquo;m going to write full articles on them soon but just as a preliminary taste here are descriptions of the two programs I released today:</p>

<ul>
<li><p>prcp.pl &ndash; Copies files and has a progress indicator</p></li>
<li><p>dirhash.pl &ndash; Hashes the names and contents of an entire directory to a single hash</p></li>
</ul>


<p>If you use them please post in the comments below.  If you want to contribute to them just send me merge requests and open tickets!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/24/mini-hack-parallel-vacuuming-in-postgresql/">Mini Hack: Parallel Vacuuming in PostgreSQL</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-24T11:42:41-05:00" pubdate data-updated="true">Jan 24<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/24/mini-hack-parallel-vacuuming-in-postgresql/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/24/mini-hack-parallel-vacuuming-in-postgresql/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I run several development environments that I need to sync with production databases to do bug fixes and new feature development.  It&rsquo;s always good to vacuum these databases before using them especially if you&rsquo;re doing it via rsync on a live system.  If you don&rsquo;t you could end up with rows that are inaccessible from your indicies and get strange results from your database.</p>

<p>Full vacuums are slow but we can&rsquo;t get around it here.  What I noticed is that normally in production vacuuming the database is an I/O bound operation but in development where we&rsquo;re working with dedicated development machines with tons of RAM we typically end up with a lot of the database for our smaller projects (&lt; 5 GB) in the cache.  This makes vacuuming a CPU bound process again and where there&rsquo;s a CPU bound process there&rsquo;s usually room for parallelism.</p>

<p>Today I decided to test out how well <a href="http://www.gnu.org/software/parallel/">GNU Parallel</a> could speed up my development machine&rsquo;s vacuuming process and I&rsquo;m happy to report that it cuts it nearly in half.  If you&rsquo;re running into the same issue and waiting for vacuuming is eating into your development time try this one liner (make sure you have <a href="http://www.gnu.org/software/parallel/">GNU Parallel</a> installed first):</p>

<pre><code>echo "\dt" | psql DB_NAME | head -n -2 | tail -n +4 | awk ' { print $3 } ' | parallel -I {} vacuumdb -f -v -d DB_NAME -t {}
</code></pre>

<p>What this does is:</p>

<ul>
<li><p>Sends the string &ldquo;\dt&rdquo; to PostgreSQL to list the table names</p></li>
<li><p>Pipes that through head and removes the last two lines (they don&rsquo;t contain table names)</p></li>
<li><p>Pipes that through tail and removes the first four lines (they also don&rsquo;t contain table names)</p></li>
<li><p>Pipes that to awk and extracts the third field (the table name)</p></li>
<li><p>Pipes that to parallel, runs vacuumdb with a full vacuum (-f) in verbose mode (-v), placing the table names where we included the curly bracket pair</p></li>
</ul>


<p>UPDATE: Ole Tange, the author of the fantastic GNU Parallel package, wrote in with his own one-liner to do the same thing as mine.  His is a bit shorter and requires fewer pipes.  Take a look:</p>

<pre><code>sql -n --list-tables pg:///DB_NAME | parallel -j0 -r --colsep '\|' sql pg:///DB_NAME vacuum full verbose {2}
</code></pre>

<p>What his does is:</p>

<ul>
<li><p>Gets a table list from GNU sql (which I had never used before, it&rsquo;s great to know that it exists!)</p></li>
<li><p>Pipes that to GNU Parallel specifying it should run as many jobs as the machine has cores (-j0), should not run if there is no input (-r), and uses the pipe character as a column separator</p></li>
<li><p>GNU Parallel then calls GNU sql, connects to the proper database executes a full, verbose vacuum on the second field it extracted from the table list (the table name)</p></li>
</ul>


<p>I added in the &ldquo;full verbose&rdquo; to Ole&rsquo;s example so the two scripts are doing the same work instead of just a plain vacuum.</p>

<p>Compare that against the run time for a normal vacuum and report your results in the comments.  For databases that won&rsquo;t fit in your RAM it may not help that much but I&rsquo;d like to hear either way.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/16/how-to-set-up-multi-head-x-in-debian-using-nvidia-cards/">How-To: Set Up Multi-head X in Debian Using Nvidia Cards</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-16T12:53:52-05:00" pubdate data-updated="true">Jan 16<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/16/how-to-set-up-multi-head-x-in-debian-using-nvidia-cards/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/16/how-to-set-up-multi-head-x-in-debian-using-nvidia-cards/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I just had to set up a multi-head system in Debian and it consumed way too much of my time.  I did see a lot of articles about it online but they all had a few common threads.</p>

<p>First, they all mentioned Xinerama was deprecated which initially scared me off of it.  Xinerama is deprecated but there&rsquo;s no replacement so for now I&rsquo;m using it until it&rsquo;s replacement is mature.  Wayland is a system that keeps getting mentioned but seems too far off to be considered for me.</p>

<p>Second, they didn&rsquo;t clearly outline how they determined all of their display information.  Some were better than others but I need to build a real list of commands to get things done properly and repeatably since I know I will have to do this again at some point.</p>

<p>So, let&rsquo;s go over what you need to do and how I got to a working system with three side-by-side displays.</p>

<p>My system consists of two Nvidia GTX 470 cards.  Each card has two DVI outputs but I only have three displays.  Therefore one card has two displays connected to it (the left and right displays) and another card has one display connected to it (the center display).</p>

<p>What you&rsquo;ll want to do is first figure out which display is connected to which card and how Linux enumerates them in your system.  To do this you should be in text mode (not X!) and go to the &ldquo;/sys/class/drm&rdquo; directory.  Let&rsquo;s take a look at mine:</p>

<pre><code>tim@desktop:/sys/class/drm$ ls -la
total 0
drwxr-xr-x  2 root root    0 Jan 15 19:21 .
drwxr-xr-x 40 root root    0 Jan 15 19:21 ..
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card0 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card0-DVI-I-1 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0/card0-DVI-I-1
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card0-DVI-I-2 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0/card0-DVI-I-2
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card0-HDMI-A-1 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0/card0-HDMI-A-1
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card1 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card1-DVI-I-3 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1/card1-DVI-I-3
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card1-DVI-I-4 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1/card1-DVI-I-4
lrwxrwxrwx  1 root root    0 Jan 15 19:21 card1-HDMI-A-2 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1/card1-HDMI-A-2
lrwxrwxrwx  1 root root    0 Jan 15 19:21 controlD64 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/controlD64
lrwxrwxrwx  1 root root    0 Jan 15 19:21 controlD65 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/controlD65
lrwxrwxrwx  1 root root    0 Jan 15 19:21 ttm -&gt; ../../devices/virtual/drm/ttm
-r--r--r--  1 root root 4096 Jan 15 19:21 version
</code></pre>

<p>Here you can see that I have two cards named card0 and card1.  Card 0 has three connectors on it named card0-DVI-I-1, card0-DVI-2, and card0-HDMI-A-1.  Card 1 has three connectors on it named card1-DVI-3, card1-DVI-4, card1-HDMI-A-2.  We will need this information later to build our xorg.conf so keep it handy.</p>

<p>Now you&rsquo;ll want to determine which monitor is hooked up to which card and port.  If you are using this setup in Windows already you are not guaranteed that the cards are recognized in the same order so just forget what Windows has told you.  To figure out which is which first make sure all of your displays are connected to your cards.  Then ask Linux which displays are connected by executing this command:</p>

<pre><code>ls /sys/class/drm/*/status | xargs -I {} -i bash -c "echo -n {}: ; cat {}"
</code></pre>

<p>This will show you which connectors think that they have something connected to them.  My output looks like this:</p>

<pre><code>tim@desktop:/sys/class/drm$ ls /sys/class/drm/*/status | xargs -I {} -i bash -c "echo -n {}: ; cat {}"
/sys/class/drm/card0-DVI-I-1/status:connected
/sys/class/drm/card0-DVI-I-2/status:connected
/sys/class/drm/card0-HDMI-A-1/status:disconnected
/sys/class/drm/card1-DVI-I-3/status:connected
/sys/class/drm/card1-DVI-I-4/status:disconnected
/sys/class/drm/card1-HDMI-A-2/status:disconnected
</code></pre>

<p>So now I know that card0 is the card that has my two displays attached to it and card1 is the card that has my two displays attached to it.  That tells me that &ldquo;DVI-I-3&rdquo; is my center display and &ldquo;DVI-I-1&rdquo; and &ldquo;DVI-I-2&rdquo; are my left and right displays.  We&rsquo;ll figure out which is left and which is right in a minute.</p>

<p>For now, let&rsquo;s figure out which PCI device Linux considers these devices.  You can do this by running this command:</p>

<pre><code>ls -la /sys/class/drm/card?
</code></pre>

<p>My output looks like this:</p>

<pre><code>tim@desktop:~$ ls -la /sys/class/drm/card?
lrwxrwxrwx 1 root root 0 Jan 15 19:21 /sys/class/drm/card0 -&gt; ../../devices/pci0000:00/0000:00:01.0/0000:01:00.0/drm/card0
lrwxrwxrwx 1 root root 0 Jan 15 19:21 /sys/class/drm/card1 -&gt; ../../devices/pci0000:00/0000:00:03.0/0000:02:00.0/drm/card1
</code></pre>

<p>Look at the numbers directly before &ldquo;drm&rdquo;.  They are &ldquo;0000:01:00.0&rdquo; for card 0 and &ldquo;0000:02:00.0&rdquo; for card 1.  This tells us that card 0 is PCI device 1 and card 1 is PCI device 2.  Keep these numbers handy too.</p>

<p>The next step is to figure out which connectors is the left and which is the right.  You can do this by guessing but let&rsquo;s just get a definitive answer.  Type the same command we used before to get the list of all of our displays (or press the up arrow if your shell supports that to get to the last command) but don&rsquo;t run it yet.  Now disconnect one display, press enter, and reconnect it.  I suggest you do this so that just in case you disconnect the display that your terminal is running on you can still run the command and see the results when you reconnect it.</p>

<p>In my case I got this output when my right display was disconnected:</p>

<pre><code>tim@desktop:/sys/class/drm$ ls /sys/class/drm/*/status | xargs -I {} -i bash -c "echo -n {}: ; cat {}"
/sys/class/drm/card0-DVI-I-1/status:disconnected
/sys/class/drm/card0-DVI-I-2/status:connected
/sys/class/drm/card0-HDMI-A-1/status:disconnected
/sys/class/drm/card1-DVI-I-3/status:connected
/sys/class/drm/card1-DVI-I-4/status:disconnected
/sys/class/drm/card1-HDMI-A-2/status:disconnected
</code></pre>

<p>Now I know that &ldquo;DVI-I-1&rdquo; is my right display since before it was connected and when I ran the command it was disconnected.  By elimination I know &ldquo;DVI-I-2&rdquo; is my right display.  If you have more displays you&rsquo;ll need to do this a few more times.</p>

<p>Let&rsquo;s recap.  I know that card 0 has two displays on it, its DVI-I-1 connector is my right display, its DVI-I-2 connector is my left display.  Card 1 has one display on it and its DVI-I-3 connector is my center display.  With that information I can start building my xorg.conf file.  I&rsquo;ll show you what I ended up with and then break it out and explain it:</p>

<pre><code>Section "ServerLayout"
    Identifier  "X.org Configured"
    Option      "Xinerama"            "on"
    Screen      0  "Screen0" 0 0
    Screen      1  "Screen1" RightOf  "Screen0"
    Screen      2  "Screen2" LeftOf   "Screen0"
    InputDevice    "Mouse0"           "CorePointer"
    InputDevice    "Keyboard0"        "CoreKeyboard"
EndSection

Section "Files"
    ModulePath   "/usr/lib/xorg/modules"
    FontPath     "/usr/share/fonts/X11/misc"
    FontPath     "/usr/share/fonts/X11/cyrillic"
    FontPath     "/usr/share/fonts/X11/100dpi/:unscaled"
    FontPath     "/usr/share/fonts/X11/75dpi/:unscaled"
    FontPath     "/usr/share/fonts/X11/Type1"
    FontPath     "/usr/share/fonts/X11/100dpi"
    FontPath     "/usr/share/fonts/X11/75dpi"
    FontPath     "/var/lib/defoma/x-ttcidfont-conf.d/dirs/TrueType"
    FontPath     "built-ins"
EndSection

Section "Module"
    Load  "glx"
    Load  "dbe"
    Load  "dri"
    Load  "record"
    Load  "dri2"
    Load  "extmod"
EndSection

Section "InputDevice"
    Identifier  "Keyboard0"
    Driver      "kbd"
EndSection

Section "InputDevice"
    Identifier  "Mouse0"
    Driver      "mouse"
    Option      "Protocol" "auto"
    Option      "Device" "/dev/input/mice"
    Option      "ZAxisMapping" "4 5 6 7"
EndSection

Section "Device"
    Identifier  "Device1"
    Driver      "nouveau"
    BusID       "PCI:1:0:0"
    Option      "ZaphodHeads" "DVI-I-1"
    Screen      0
EndSection

Section "Device"
    Identifier  "Device2"
    Driver      "nouveau"
    BusID       "PCI:1:0:0"
    Option      "ZaphodHeads" "DVI-I-2"
    Screen      1
EndSection

Section "Device"
    Identifier  "Device0"
    Driver      "nouveau"
    BusID       "PCI:2:0:0"
    Option      "ZaphodHeads" "DVI-I-3"
    Screen      0
EndSection

Section "Screen"
    Identifier  "Screen0"
    Device      "Device0"
EndSection

Section "Screen"
    Identifier  "Screen1"
    Device      "Device1"
EndSection

Section "Screen"
    Identifier  "Screen2"
    Device      "Device2"
EndSection
</code></pre>

<p>Here&rsquo;s the explanation of each section:</p>

<pre><code>Section "ServerLayout"
    Identifier  "X.org Configured"
    Option      "Xinerama"            "on"
    Screen      0  "Screen0" 0 0
    Screen      1  "Screen1" RightOf  "Screen0"
    Screen      2  "Screen2" LeftOf   "Screen0"
    InputDevice    "Mouse0"           "CorePointer"
    InputDevice    "Keyboard0"        "CoreKeyboard"
EndSection
</code></pre>

<p>This is the server layout section that declares how the screens are laid out and what input devices we&rsquo;re using.  It specifies a name of &ldquo;X.org Configured&rdquo; since I used another configuration as a baseline for this.  You can change that name to whatever you&rsquo;d like.  Then it specifies that Xinerama is on.  This means that you can drag windows back and forth between screens.  It has some downsides but they are outweighed by the convenience of placing windows anywhere across all of your displays.</p>

<p>Then it specifies three &ldquo;screen&rdquo; entries.  Later on &ldquo;Screen0&rdquo; will by my center screen.  &ldquo;0 0&rdquo; just means that that screen starts at pixel 0,0.  &ldquo;Screen1&rdquo; will be might right screen so it will be to the right of my center screen.  In xorg.conf language that is done with &ldquo;RightOf&rdquo;.  &ldquo;Screen2&rdquo; will be my left screen so it is &ldquo;LeftOf&rdquo; my center screen.</p>

<pre><code>Section "Files"
    ModulePath   "/usr/lib/xorg/modules"
    FontPath     "/usr/share/fonts/X11/misc"
    FontPath     "/usr/share/fonts/X11/cyrillic"
    FontPath     "/usr/share/fonts/X11/100dpi/:unscaled"
    FontPath     "/usr/share/fonts/X11/75dpi/:unscaled"
    FontPath     "/usr/share/fonts/X11/Type1"
    FontPath     "/usr/share/fonts/X11/100dpi"
    FontPath     "/usr/share/fonts/X11/75dpi"
    FontPath     "/var/lib/defoma/x-ttcidfont-conf.d/dirs/TrueType"
    FontPath     "built-ins"
EndSection

Section "Module"
    Load  "glx"
    Load  "dbe"
    Load  "dri"
    Load  "record"
    Load  "dri2"
    Load  "extmod"
EndSection
</code></pre>

<p>I left these two sections completely stock.  They&rsquo;re magic for now so don&rsquo;t make any changes to them.</p>

<pre><code>Section "InputDevice"
    Identifier  "Keyboard0"
    Driver      "kbd"
EndSection

Section "InputDevice"
    Identifier  "Mouse0"
    Driver      "mouse"
    Option      "Protocol" "auto"
    Option      "Device" "/dev/input/mice"
    Option      "ZAxisMapping" "4 5 6 7"
EndSection
</code></pre>

<p>These two sections specify our input devices.  These are stock also so you can leave them alone too.</p>

<pre><code>Section "Device"
    Identifier  "Device1"
    Driver      "nouveau"
    BusID       "PCI:1:0:0"
    Option      "ZaphodHeads" "DVI-I-1"
    Screen      0
EndSection
</code></pre>

<p>Here&rsquo;s some more configuration we have to do.  Remember that DVI-I-1 is our right display and we are calling it &ldquo;Screen1&rdquo;.  In this section we need to create a device for it that tells X which drive, card, and connector it uses as well as which screen it is relative to its specific graphics card.  We&rsquo;ll call it &ldquo;Device1&rdquo;, use the open-source Nvidia driver called &ldquo;Nouveau&rdquo;, tell it to use PCI device 1 (since card 0 was device 1) with the &ldquo;PCI:1:0:0&rdquo; string, then tell it the connector is &ldquo;DVI-I-1&rdquo; with the &ldquo;ZaphodHeads&rdquo; option, and finally that this is the first display (display 0) on this graphics card.</p>

<pre><code>Section "Device"
    Identifier  "Device2"
    Driver      "nouveau"
    BusID       "PCI:1:0:0"
    Option      "ZaphodHeads" "DVI-I-2"
    Screen      1
EndSection
</code></pre>

<p>For our left display we have similar settings but the connector is &ldquo;DVI-I-2&rdquo; and this is the second display (display 1) on this graphics card.  We also call this &ldquo;Device2&rdquo;.</p>

<pre><code>Section "Device"
    Identifier  "Device0"
    Driver      "nouveau"
    BusID       "PCI:2:0:0"
    Option      "ZaphodHeads" "DVI-I-3"
    Screen      0
EndSection
</code></pre>

<p>Our center screen is &ldquo;Device0&rdquo; and it is on a different graphics card.  Remember card 1 was PCI device 2 so we use &ldquo;PCI:2:0:0&rdquo; here and specify the connector as &ldquo;DVI-I-3&rdquo;.  This is the first display (display 0) on this graphics card.</p>

<pre><code>Section "Screen"
    Identifier  "Screen0"
    Device      "Device0"
EndSection

Section "Screen"
    Identifier  "Screen1"
    Device      "Device1"
EndSection

Section "Screen"
    Identifier  "Screen2"
    Device      "Device2"
EndSection
</code></pre>

<p>Finally, these sections map our devices to name screens.  In this case DeviceX is ScreenX.  Nothing fancy goes on here.</p>

<p>Replace or create your xorg.conf using the steps you see here and you should be up and running in no time.  I had to change from Gnome to Xfce since Gnome had a really tall black bar that took over my center display that I couldn&rsquo;t get rid of.  If you run into the same problem try Xfce or another window manager instead.</p>

<p>Good luck and post in the comments if you found this useful or need some assistance.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/10/how-to-find-and-fix-duplicate-java-object-serial-numbers/">How-To: Find and Fix Duplicate Java Object Serial Numbers</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-10T09:43:03-05:00" pubdate data-updated="true">Jan 10<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/10/how-to-find-and-fix-duplicate-java-object-serial-numbers/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/10/how-to-find-and-fix-duplicate-java-object-serial-numbers/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In Java making your objects serializable is often required to work with certain libraries (<a href="http://en.wikipedia.org/wiki/Object-relational_mapping">ORM</a>, <a href="http://code.google.com/webtoolkit/doc/latest/tutorial/RPC.html">GWT-RPC</a>, etc).  It&rsquo;s tempting when Eclipse tells us that our object is missing a unique identifier to have it generate one on the fly.  However, in practice what happens sometimes in the field is that you&rsquo;ll create a serializable class and then copy it to make a new class.  When you do this you&rsquo;re inadvertently copying the objects &ldquo;serialVersionUID&rdquo; field.</p>

<p>Why is this an issue?  Well, the serialVersionUID is supposed to be a universal identifier.  If it&rsquo;s not universal then you&rsquo;re asking for trouble.  When you deserialize a new version of this object or try to deserialize an object to the wrong object type you can end up with strange results or have your application crash altogether.</p>

<p>How do we fix it?  I&rsquo;ve written a command-line one liner (even though it looks like three lines on the site) that will print out a list of the files that have duplicated serialVersionUID values.  Here it is:</p>

<pre><code>grep -r serialVersionUID * | grep -o "private.*" | sort | uniq -d | sed 's/^.*=//' | sed 's/;//' | sed 's/^ //' | sed 's/^-/\\\\-/' | xargs -I PATTERNS grep -r PATTERNS . | cut -f1 -d ':'
</code></pre>

<p>Go into each of these files, remove their serialVersionUID field, and regenerate them.  Don&rsquo;t concern yourself with which ones overlap unless you already have serialized versions of these objects lying around.  If you do have serialized versions you should remove the field and reserialize your objects into your data store without it.  Then you can try to generate new values and again reserialize them into the data store.  DO NOT do this in production, always do it in a test environment to make sure that nothing breaks.  You do have unit tests that&rsquo;ll test these things, right?  :)</p>

<p>Post in the comments if you find this useful and keep hacking!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2012/01/09/mini-hack-approximating-the-size-of-a-java-object/">Mini Hack: Approximating the Size of a Java Object</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-01-09T16:31:08-05:00" pubdate data-updated="true">Jan 9<span>th</span>, 2012</time>
        
           | <a href="/archives/2012/01/09/mini-hack-approximating-the-size-of-a-java-object/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2012/01/09/mini-hack-approximating-the-size-of-a-java-object/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Finding the size of an object or data structure used to be simple in &ldquo;the good old days&rdquo;.  You could just use the sizeof function and it&rsquo;d spit out something that usually made sense.  In Java there&rsquo;s nothing that seems to do it quite that simply.  I&rsquo;ve seen some solutions that involve using Java instrumentation but they have interesting quirks that can confuse things considerably.  For example, using it to obtain the size of a String object will always show 32 bytes as I found out by reading a thread on StackOverflow.</p>

<p>In any case I wanted something simple that would do the job.  My requirements were that there was minimal or zero setup, results were fairly close to the actual memory footprint, and that it work consistently unless I was using some kind of very strange data structure.  Here&rsquo;s what I came up with:</p>

<pre><code>try {
  // Get a ByteArrayOutputStream to catch the output of the
  //   ObjectOutputStream without going to disk
  ByteArrayOutputStream baos = new ByteArrayOutputStream();

  // Get an ObjectOutputStream so we can dump the entire
  //   object at one shot
  ObjectOutputStream oos = new ObjectOutputStream(baos);

  // Write the object
  oos.writeObject(o);

  // Close the stream
  oos.close();

  // Query the ByteArrayOutputStream for its size
  return baos.size();
} catch (IOException e) {
  // Something went wrong.  Print the stack trace.
  e.printStackTrace();

  // Return -1 so the caller knows we failed
  return -1;
}
</code></pre>

<p>Put this into your standard utility class as a static method and call it whenever you&rsquo;re curious about memory usage.  The output of this should be a little higher than the actual in-memory usage so you can consider this a high water mark for your object&rsquo;s size.</p>

<p>Post in the comments if you end up using it!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2011/12/26/how-to-disable-hdfs-permissions-for-hadoop-development/">How-To: Disable HDFS Permissions for Hadoop Development</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-12-26T11:47:35-05:00" pubdate data-updated="true">Dec 26<span>th</span>, 2011</time>
        
           | <a href="/archives/2011/12/26/how-to-disable-hdfs-permissions-for-hadoop-development/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2011/12/26/how-to-disable-hdfs-permissions-for-hadoop-development/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>If you&rsquo;ve <a href="http://blog.timmattison.com/archives/2011/12/23/how-to-install-hadoop-on-debian-ubuntu/">set up Hadoop for development</a> you may be wondering why you can&rsquo;t read or write files or create MapReduce jobs then you&rsquo;re probably missing a tiny bit of configuration.  For most development systems in pseudo-distributed mode it&rsquo;s easiest to disable permissions altogether.  This means that any user, not just the &ldquo;hdfs&rdquo; user, can do anything they want to HDFS so do not do this in production unless you have a very good reason.</p>

<p>The error message you&rsquo;re most likely seeing if permissions are the problem is similar to this:</p>

<p><code>
put: org.apache.hadoop.security.AccessControlException: Permission denied: user=tim, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x
</code></p>

<p>If that&rsquo;s the case and you really want to disable permissions just add this snippet into your hdfs-site.xml file (located in /etc/hadoop-0.20/conf.empty/hdfs-site.xml on Debian Squeeze) in the configuration section:</p>

<pre><code>  &lt;property&gt;
    &lt;name&gt;dfs.permissions&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
</code></pre>

<p>Then restart Hadoop (su to the &ldquo;hdfs&rdquo; user and run bin/stop-all.sh then bin/start-all.sh) and try putting a file again.  You should now be able to read/write with no restrictions.</p>

<p>Good luck!  Post in the comments if it doesn&rsquo;t work for you.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2011/12/23/how-to-install-hadoop-on-debian-ubuntu/">How-To: Install Hadoop on Debian/Ubuntu</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-12-23T11:19:43-05:00" pubdate data-updated="true">Dec 23<span>rd</span>, 2011</time>
        
           | <a href="/archives/2011/12/23/how-to-install-hadoop-on-debian-ubuntu/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2011/12/23/how-to-install-hadoop-on-debian-ubuntu/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><em>UPDATE 2012-02-06</em>: <strong>The native library issue has been resolved</strong>.  You can now keep the native library installed with just a minor configuration tweak.  If you have already followed these instructions just scroll down to the core-site.xml contents and the commands to make the &ldquo;/hadoop&rdquo; directory on your machine.  Those two things will get you up to date.</p>

<p>Yesterday I passed my <a href="http://www.cloudera.com/hadoop-training/developer-training/">Cloudera Certified Hadoop Developer (CCHD)</a> certification.  The four day class was great and I learned a lot about everything from the architecture of HDFS all the way to specific MapReduce algorithms and how to implement them using Hadoop.</p>

<p>I&rsquo;m excited about implementing this in my companies as we&rsquo;re already using a home grown form of MapReduce for certain processes.  We chose MapReduce for ease of development as the algorithms tend to be surprisingly simple for what you get back.  This makes our system easier to debug and now with Hadoop it makes it as scalable as we can afford.</p>

<p>I wanted to get started with using Hadoop on our standard development environment which happens to be Debian Squeeze and not CentOS as the Cloudera course uses.  Luckily they provide a Debian package repository that makes setting up your development environment pretty simple.  This setup will <em>only</em> be suitable for production since you&rsquo;ll be running a single node.  In later posts I&rsquo;ll start discussing how to set up a testing cluster and eventually give some tips on working with Hadoop in production but for now lets just focus on development.</p>

<p>The first thing you&rsquo;ll want to do is to edit your apt sources in /etc/apt/sources.list.  At the end of your existing configuration add the following lines:</p>

<pre><code>deb http://archive.cloudera.com/debian squeeze-cdh3 contrib
deb-src http://archive.cloudera.com/debian squeeze-cdh3 contrib
</code></pre>

<p>Note: This configuration is specifically for a Debian Squeeze installation.  If you are using a different release like Debian Lenny or Ubuntu Karmic you&rsquo;ll need to change &ldquo;squeeze-cdh3&rdquo; to match your setup.  You can <a href="http://archive.cloudera.com/debian/dists/">look into the repository to get a list of valid values</a> but usually it&rsquo;s just the release name followed by cdh3 (ie. &ldquo;lenny-cdh3&rdquo; or &ldquo;karmic-cdh3&rdquo;).</p>

<p>Next add Cloudera&rsquo;s public key into your system by running this command (don&rsquo;t forget the dash on the end of the command, it is required!):</p>

<pre><code>curl -s http://archive.cloudera.com/debian/archive.key | sudo apt-key add -
</code></pre>

<p>Now you&rsquo;ll want to update your package lists so that your OS knows the new packages exist.  You can do this by simply running:</p>

<pre><code>sudo apt-get update
</code></pre>

<p>Now that you have an updated package list you can go about installing Hadoop.  For a standalone, single node development installation you&rsquo;ll need the following pieces:</p>

<ul>
<li><p>Hadoop 0.20 &ndash; The core components of Hadoop (the JARs, Python scripts, documentation, etc) but no daemons</p></li>
<li><p>The namenode daemon &ndash; To provide access to your HDFS volume</p></li>
<li><p>The datanode daemon &ndash; To store your HDFS data</p></li>
<li><p>The jobtracker daemon &ndash; To schedule and hand out jobs</p></li>
<li><p>The tasktracker daemon &ndash; To poll the job tracker, and accept and run Hadoop jobs</p></li>
</ul>


<p>To get all of that you&rsquo;ll need to run the following command:</p>

<pre><code>sudo apt-get install hadoop-0.20 hadoop-0.20-namenode hadoop-0.20-datanode hadoop-0.20-jobtracker hadoop-0.20-tasktracker
</code></pre>

<p>Now you have successfully installed Hadoop and the necessary libraries to run a single HDFS node that can run Hadoop jobs.  In single node mode you don&rsquo;t get any of the benefits of HDFS (block replication) and your Hadoop jobs won&rsquo;t run terribly fast but you&rsquo;ll be able to develop and test your code.</p>

<p>For development purposes your system is now running in standalone mode.  This means that if you run HDFS commands (ie, hadoop fs -ls) you will not be connecting to HDFS but instead you will be looking at your local file system relative to where you ran the &ldquo;hadoop&rdquo; command.  With this setup you can run local jobs and kick the tires so if that&rsquo;s all you need you&rsquo;re done.</p>

<p>I would personally recommend that you run in pseudo-distributed mode if you intend to eventually move onto a production cluster and will be involved at least in some way in its administration.  This is not to say that you&rsquo;ll be the administrator of the cluster but that you&rsquo;ll at least be one of the people called upon to figure out production problems that range from HDFS issues to failed or buggy jobs.</p>

<p>In pseudo-distributed mode you will really be running jobs through the entire Hadoop workflow and will be able to tell whether your jobs will run on a cluster or not.  There are still some gotchas that differ from running fully distributed vs. pseudo-distributed but if you follow best practices and defensive coding you can usually avoid them.  For example you should never depend on local state or the availability of files on the local file system.  Always read through HDFS if it&rsquo;s necessary and never use globals in your code that could be modified, or even accessed, between map and reduce tasks.  Now onto pseudo-distributed mode&hellip;</p>

<p>Cloudera has done a lot for you up to this point including applying hundreds of patches that you&rsquo;ll be glad you don&rsquo;t need to worry about but now you&rsquo;ll need to do some configuration.  This is intentional since you can use CDH to run anything from a development node to a production cluster.  Therefore they don&rsquo;t make any assumptions as to how you want the nodes configured.  Being consistent in development and production and you&rsquo;ll make your life a lot easier when you need to debug something as everyone will be familiar with the same layout, major/minor release number, etc.</p>

<p>Let&rsquo;s take care of the configuration steps one-by-one now.  The steps are:</p>

<ul>
<li><p>Set JAVA_HOME in the hadoop-env.sh script</p></li>
<li><p>Configure core-site.xml</p></li>
<li><p>Configure hdfs-site.xml</p></li>
<li><p>Configure mapred-site.xml</p></li>
<li><p>Setup passphraseless ssh</p></li>
<li><p>Format the namenode</p></li>
</ul>


<p>Once that is complete you can start all of the required processes and begin testing but keep reading for a walkthrough of each of these steps.</p>

<p>Let&rsquo;s make sure JAVA_HOME is set in your hadoop-env.sh script.  On Debian Squeeze this is located in /etc/hadoop-0.20/conf.empty/hadoop-env.sh.  By default JAVA_HOME is NOT set in hadoop-env.sh.  If you have it set in your profile already I would suggest copying that export line to hadoop-env.sh just in case you run the daemons later as a different user that might not have the same profile.  If you don&rsquo;t know what your JAVA_HOME value should be you can run this one-liner (<a href="http://serverfault.com/questions/143786/how-to-determine-java-home-on-debian-ubuntu">credits to a thread on ServerFault for this one</a>):</p>

<pre><code>export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
</code></pre>

<p>To verify that it is set run:</p>

<pre><code>export | grep JAVA_HOME
</code></pre>

<p>And then list the contents of the directory that it references.  It should contain a bin, lib, and man directory.  If so, copy the entire export line into the beginning of the hadoop-env.sh script where it has been commented out.  If they have the same JAVA_HOME specified you can just uncomment that line.  Now whomever is starting the Hadoop processes (you, the hdfs user, or root) will always have JAVA_HOME set properly.</p>

<p>Guidance for the next few steps was taken from the <a href="http://hadoop.apache.org/common/docs/r0.20.2/quickstart.html#Configuration">Hadoop Quick Start&rsquo;s &ldquo;Pseudo-Distributed Operation: Configuration&rdquo; section</a>.  Refer there for additional information.  I have just added some narrative to let you know what each of these steps does so you understand a bit more about how the system works.</p>

<p>Note: The XML files are owned by root and are accessible by the hadoop group.  Edit the files as root but run any commands below as the &ldquo;hdfs&rdquo; user.</p>

<p>core-site.xml (located in /etc/hadoop-0.20/conf.empty/core-site.xml on Debian Squeeze) needs to be modified to let the everyone know where the name node is running.  In our case it will be running on localhost on the default port so add this snippet of XML between the configuration tags in core-site.xml:</p>

<pre><code>  &lt;property&gt;
    &lt;name&gt;fs.default.name&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/hadoop/hadoop-${user.name}&lt;/value&gt;
  &lt;/property&gt;
</code></pre>

<p>After adding these settings you&rsquo;ll need to make sure that the /hadoop base directory has been created with the proper permissions to let all users get access to it.  In production your security criteria may be different so only use this for development machines since it leaves everything pretty open!  To create the directory you&rsquo;ll want to do the following:</p>

<pre><code>sudo mkdir /hadoop &amp;&amp; sudo chown hdfs:hdfs /hadoop &amp;&amp; sudo chmod 777 /hadoop
</code></pre>

<p>Now you will have the /hadoop path created on permanent storage instead of it getting placed on tmpfs where the default values would normally place it.</p>

<p>hdfs-site.xml (located in /etc/hadoop-0.20/conf.empty/hdfs-site.xml on Debian Squeeze) needs to be modified to tell the HDFS daemons that we only want a replication factor of 1.  This doesn&rsquo;t matter so much since HDFS won&rsquo;t try to replicate the same block multiple times on one data node but if you start running multiple data nodes on a single development machine it will save you some disk space.  So put this snippet of XML between the configuration tags in hdfs-site.xml:</p>

<pre><code>  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
</code></pre>

<p>mapred-site.xml (located in /etc/hadoop-0.20/conf.empty/mapred-site.xml on Debian Squeeze) needs to be modified to let everyone know where the job tracker is running.  In our case it will be running on localhost on the default port so add this snippet of XML between the configuration tags in mapred-site.xml:</p>

<pre><code>  &lt;property&gt;
    &lt;name&gt;mapred.job.tracker&lt;/name&gt;
    &lt;value&gt;localhost:9001&lt;/value&gt;
  &lt;/property&gt;
</code></pre>

<p>As root you&rsquo;ll need to run two commands to make sure that the rest of the config will work as the &ldquo;hdfs&rdquo; user.  You need to create a directory for the &ldquo;hdfs&rdquo; user to store its SSH credentials.  To do this run the following commands (again, as root):</p>

<pre><code>mkdir /usr/lib/hadoop-0.20/.ssh
chown hdfs:hdfs /usr/lib/hadoop-0.20/.ssh
</code></pre>

<p>This is assuming that your hadoop directory is /usr/lib/hadoop-0.20.  To verify where it is if you aren&rsquo;t sure su to the &ldquo;hdfs&rdquo; user and run &ldquo;pwd&rdquo;.  That will be the directory where this should be done.</p>

<p>Remember to run all of the steps from here as the &ldquo;hdfs&rdquo; user or it will not work.</p>

<p>Passphraseless SSH is required so that the scripts can connect to servers and start/stop services in a cluster.  To make sure you can ssh without a password localhost first try this:</p>

<pre><code>ssh localhost
</code></pre>

<p>If you are prompted for a password it has not been set up yet.  To set it up you&rsquo;ll need to generate a public/private key pair and then tell SSH to accept the public key as a valid login credential.  Do this by running the following commands:</p>

<pre><code>ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<p>Try to ssh to localhost now and make sure that it works.  <strong><em>If you skip this step the host&rsquo;s key will not be added to the list of known hosts and the non-interactive processes will fail</em></strong>.</p>

<p>If the ssh-keygen command prompts you to overwrite your key <strong><em>DO NOT DO IT</em></strong>!  This will break any existing key-based SSH connections have already.  Just say no if asked to overwrite your key.  Adding an existing key to the list of authorized keys will still work.</p>

<p>The last configuration step is to format the name node.  This will make sure the name node has all of its data structures on disk set up properly so that it can track your HDFS data.  To do this just run the following hadoop (located in /usr/bin/hadoop on Debian Squeeze) command:</p>

<pre><code>hadoop namenode -format
</code></pre>

<p>If you are asked to reformat the filesystem you must make sure that nobody else is already running Hadoop on this machine.  If they are and you reformat the name node you will end up wiping out their data.  It is recoverable but you don&rsquo;t want to have to go through that as it is time consuming and not guaranteed to work.</p>

<p>Now that everything is set up you can start everything by running the start-all.sh (located in /usr/lib/hadoop-0.20/bin/start-all.sh on Debian Squeeze) script:</p>

<pre><code>start-all.sh
</code></pre>

<p>If you didn&rsquo;t see any errors you should be up and running.  Here are some things that we should verify to make sure that everything works as expected:</p>

<ul>
<li><p>Run &ldquo;jps&rdquo; and verify all of the processes are there</p></li>
<li><p>Test the name node&rsquo;s web GUI</p></li>
<li><p>Test the job tracker&rsquo;s web GUI</p></li>
<li><p>Test HDFS</p></li>
<li><p>Run a Hadoop MapReduce example</p></li>
</ul>


<p>When you run &ldquo;jps&rdquo; you should get a list of processes including &ldquo;TaskTracker&rdquo;, &ldquo;JobTracker&rdquo;, &ldquo;SecondaryNameNode&rdquo;, &ldquo;DataNode&rdquo;, and &ldquo;NameNode&rdquo;.  If any of these are missing check the log directory.  On my first run the name node failed to come up and it was because I had manually created the /tmp/hadoop-hdfs/dfs/name directory when doing some earlier testing.  After that the name node format failed silently.  If you get an exception that reads &ldquo;java.io.IOException: NameNode is not formatted.&rdquo; you should run stop-all.sh, then remove the /tmp/hadoop-hdfs directory (again, make sure nobody else is running Hadoop on your machine!), and then reformat the name node and restart the processes with start-all.sh.</p>

<p>Now that all of the processes are running check that the <a href="http://localhost:50030">job tracker web GUI</a> and the <a href="http://localhost:50070">name node web GUI</a> are running.  If both of those links bring up web pages without errors then they are both running properly.</p>

<p>Let&rsquo;s test HDFS with some simple commands.  You should still be the &ldquo;hdfs&rdquo; user for this.  Don&rsquo;t change back to your regular user account yet.  First, list the files on the HDFS volume with this command:</p>

<pre><code>hadoop fs -ls
</code></pre>

<p>When you first run this you should get an error that reads &ldquo;ls: Cannot access .: No such file or directory.&rdquo;.  That&rsquo;s OK.  Let&rsquo;s put a file there any make sure that it works.  Generate a file full of random data with this command:</p>

<pre><code>head -c 1048576 /dev/urandom &gt; /tmp/random.txt
hadoop fs -put /tmp/random.txt random.txt
</code></pre>

<p>If those commands give you errors try <a href="http://blog.timmattison.com/archives/2011/12/26/how-to-disable-hdfs-permissions-for-hadoop-development/">disabling HDFS permissions</a>, then stop and restart everything with the stop-all.sh, and start-all.sh scripts.  Then come back here and try again.</p>

<p>Now list the files again as you did above and you should see a 1 MB file called random.txt.  Verify that the input file and the output file match by comparing their sha1sums:</p>

<pre><code>sha1sum /tmp/random.txt
hadoop fs -cat random.txt | sha1sum
</code></pre>

<p>You should get two hex strings on separate lines that match.  You can ignore the filename portion of the result since one run is taking the data from the local filesystem and the other is reading the data from HDFS and piping it to sha1sum.</p>

<p>Finally, let&rsquo;s run a MapReduce job.  There are a few example jobs included with the default installation and the simplest one is grep.  We&rsquo;ll need to build a file that we want to grep, put it on HDFS, run the job, and compare the results to running a non-MapReduce grep on it.</p>

<p>I wanted to build a file that was bigger than a single tiny text file I ran the following command (still as the &ldquo;hdfs&rdquo; user):</p>

<pre><code>find /etc -exec cat {} \; | strings &gt; /tmp/etc-dir.txt
</code></pre>

<p>This reads all of the files in the /etc directory, extracts everything that looks like a string so we don&rsquo;t end up with a binary mess, and puts it in the /tmp/etc-dir.txt.  I then put that file onto HDFS:</p>

<pre><code>hadoop fs -put /tmp/etc-dir.txt etc-dir.txt
</code></pre>

<p>Then I ran a MapReduce job that returns the number of occurrences of the string &ldquo;localhost&rdquo; in the file:</p>

<pre><code>cd /usr/lib/hadoop-0.20
hadoop jar hadoop-*-examples.jar grep etc-dir.txt etc-dir-results localhost
</code></pre>

<p>And I compared that to counting the occurrences with grep:</p>

<pre><code>hadoop fs -cat etc-dir-results/part-00000
grep localhost /tmp/etc-dir.txt | wc -l
</code></pre>

<p>To verify that it worked I should see two lines of text that both start with the same number.  The first line will contain the string &ldquo;localhost&rdquo; on the end of it, the second one will not.</p>

<p>If you got this far you&rsquo;ve now got a working Hadoop developer setup.  If not, post in the comments and I&rsquo;ll try to help out and update the tutorial with more information.  Good luck!  Hadoop is an exciting tool with lots of applications that I&rsquo;ll explore in future posts.</p>

<p>If you&rsquo;re going to get serious with it look into getting a CCHD certification.  With it you&rsquo;ll skip a few very hard months of work on your own in just four days and you&rsquo;ll come out of it with a sound understanding of HDFS, the MapReduce process, and get a lot of good tips on what you should and shouldn&rsquo;t do.  The instructors are very knowledgeable, have a large support network to answer your domain specific questions, and it can be a great networking opportunity.  If you have questions about the course post them in the comments too.</p>

<p>Update: Having trouble with permissions?  Check this other short article on <a href="http://blog.timmattison.com/archives/2011/12/26/how-to-disable-hdfs-permissions-for-hadoop-development/">how to disable permissions for a development machine</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2011/11/02/how-to-install-googles-android-eclipse-plugin-and-or-adb-on-64-bit-debian-ubuntu/">How-To: Install Google&#8217;s Android Eclipse Plugin (And/or Adb) on 64-bit Debian/Ubuntu</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-11-02T09:00:48-04:00" pubdate data-updated="true">Nov 2<span>nd</span>, 2011</time>
        
           | <a href="/archives/2011/11/02/how-to-install-googles-android-eclipse-plugin-and-or-adb-on-64-bit-debian-ubuntu/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2011/11/02/how-to-install-googles-android-eclipse-plugin-and-or-adb-on-64-bit-debian-ubuntu/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Today I had to reinstall the Android plugin on my system and I recently upgraded to a 64-bit development VM.  To my surprise the installation didn&rsquo;t go smoothly at all.  After restarting Eclipse twice I was constantly presented with two error messages &ldquo;Failed to parse the output of &lsquo;adb version&rsquo;&rdquo; and &ldquo;adb: error while loading shared libraries: libncurses.so.5: cannot open shared object file: No such file or directory&rdquo;.  Your system may also present another error message that reads &ldquo;adb: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory&rdquo;.</p>

<p>I could see that it was looking for libncurses.so.5 however I know that ncurses is already installed on my machine in /lib as /lib/libncurses.so.5.  So where exactly was Eclipse/adb looking for it?  It turns out that it wants to find its libraries in the /lib32 directory but you can&rsquo;t just symlink it or you&rsquo;ll get an error that reads &ldquo;wrong ELF class: ELFCLASS64&rdquo;.  adb needs to have the 32-bit versions installed or it won&rsquo;t function at all.</p>

<p>So to get up and running just run the following command to fix the issue:</p>

<pre><code>sudo apt-get install lib32ncurses5 lib32stdc++6
</code></pre>

<p>After that just restart Eclipse and the issue should be fully put to bed.  Let me know how it works out for you or if you run into trouble.</p>

<p>If you still run into trouble like an error message that reads &ldquo;aapt: error while loading shared libraries: libz.so.1: cannot open shared object file: No such file or directory&rdquo; you need to install the ia32-libs like this:</p>

<pre><code>sudo apt-get install ia32-libs
</code></pre>

<p>Then rebuild your project and the errors should be gone.</p>

<p>UPDATE 2012-02-14: Rortian reports that the following command words on Fedora 16:</p>

<pre><code>yum install ncurses-libs.i686 libstdc++.i686 libgcc.i686
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2011/11/01/how-to-run-teamviewer-6-as-root-in-linux-debian-ubuntu/">How-To: Run TeamViewer 6 as Root in Linux (Debian, Ubuntu)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-11-01T15:40:16-04:00" pubdate data-updated="true">Nov 1<span>st</span>, 2011</time>
        
           | <a href="/archives/2011/11/01/how-to-run-teamviewer-6-as-root-in-linux-debian-ubuntu/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2011/11/01/how-to-run-teamviewer-6-as-root-in-linux-debian-ubuntu/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>If you are reading this article I, unlike the rest of the Internet, will assume you have a good reason to run Teamviewer as root.  I won&rsquo;t spend my time trying to talk you out of it.  :)</p>

<p>You can accomplish this in just a few quick steps:</p>

<p>1) Install Teamviewer 6
2) Try to run it as root, if you get the message &ldquo;TeamViewer must not be executed as root!&rdquo; go on to step 3.  Otherwise, you are already set up.
3) Open /opt/teamviewer/teamviewer/6/bin/wrapper in your favorite text editor
4) Look for a line similar to this: &ldquo;validate_user                  # die if root&rdquo;
5) Put a single hash/octothorpe in front of the line and save it so it looks like &ldquo;#validate_user              # die if root&rdquo;.
6) Restart Teamviewer as root and you&rsquo;re good to go</p>

<p>Post in the comments if you used this tip and let me know if you run into any trouble.  This should work on other versions of Linux but the files may be in different places.  If you want to find your operating system&rsquo;s copy of &ldquo;wrapper&rdquo; try running &ldquo;locate wrapper | grep -i teamviewer&rdquo; and it should come up in the list.  If you don&rsquo;t have locate installed you can try the much slower: &ldquo;find / -name &#8220;wrapper&rdquo; | grep -i teamviewer&#8221; to do the same thing.  Good luck!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/archives/2011/09/13/mini-hack-find-java-code-abusing-stdout/">Mini Hack: Find Java Code Abusing Stdout</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-09-13T18:04:39-04:00" pubdate data-updated="true">Sep 13<span>th</span>, 2011</time>
        
           | <a href="/archives/2011/09/13/mini-hack-find-java-code-abusing-stdout/#disqus_thread"
             data-disqus-identifier="http://blog.timmattison.com/archives/2011/09/13/mini-hack-find-java-code-abusing-stdout/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I found myself recently trying to determine where a particular piece of code was that was polluting my stdout stream with hundreds of unwanted messages.  This seemed like an impossible task since I could only guess that there was a line of code somewhere that was calling System.out.println(clazz.toString()) or something similar.  I searched the codebase high and low and found nothing.</p>

<p>After some Googling I came up with an alternate approach.  I disabled all of the code I could find that printed to stdout (actually this was easy since I do it all with one function so I just disabled that) and verified that I my application was still spewing out noise.  Sure enough it was and then I added a tiny chunk of code to my application&rsquo;s initialization.  That code takes over stdout and forcibly causes a NullPointerException.  It&rsquo;s unconventional and I probably could&rsquo;ve just put in a breakpoint but I wanted to make sure it crashed and spat out something I could work with immediately.</p>

<p>Here is what I did:</p>

<pre><code>System.setOut(new PrintStream(new OutputStream() {
  @Override
  public void write(int arg0) throws IOException {
    String forcedCrash = null;
    forcedCrash = forcedCrash + " ";
  }
}));
</code></pre>

<p>Let me know in the comments if you get a chance to use this to debug a project.  Since I came up with this technique I find myself putting it in action a lot more often than I thought I would so hopefully it&rsquo;ll be useful for you too.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/page/8/">&larr; Older</a>
    
    <a href="/archives">Archives</a>
    
    <a class="next" href="/page/6/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
	<h1>Like this site/article?  Donate with Bitcoin!</h1>
	<a href="bitcoin:1PcYJnuZPZFWUMUHzzBGHHshEbrVzYpY9Y"><img src="http://blockchain.info/qr?data=1PcYJnuZPZFWUMUHzzBGHHshEbrVzYpY9Y&size=200" alt="Bitcoin"></a>
	<a href="bitcoin:1PcYJnuZPZFWUMUHzzBGHHshEbrVzYpY9Y">1PcYJnuZPZFWUMUHzzBGHHshEbrVzYpY9Y</a>
</section>
<section>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Sidebar -->
<ins class="adsbygoogle"
     style="display:inline-block;width:120px;height:240px"
     data-ad-client="ca-pub-9307090849713032"
     data-ad-slot="1616086903"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/archives/2014/12/16/mockito-and-servletinputstreams/">Mockito and ServletInputStreams</a>
      </li>
    
      <li class="post">
        <a href="/archives/2014/11/12/fixing-javac-on-mac-os-when-multiple-jvms-are-installed/">Fixing Javac on Mac OS When Multiple JVMs Are Installed</a>
      </li>
    
      <li class="post">
        <a href="/archives/2014/11/12/when-unicode-goes-wrong-in-java/">When Unicode Goes Wrong in Java</a>
      </li>
    
      <li class="post">
        <a href="/archives/2014/10/30/using-interfaces-in-camels-java-dsl-with-spring/">Using Interfaces in Camel&#8217;s Java DSL With Spring</a>
      </li>
    
      <li class="post">
        <a href="/archives/2014/10/28/activating-u2f-on-a-yubikey-neo-on-mac-os/">Activating U2F on a Yubikey Neo on Mac OS</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/timmattison">@timmattison</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'timmattison',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/timmattison?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Tim Mattison -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'timmattison';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>





  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
